{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Seq2seq with attention\n",
    "- Evaluation metric\n",
    "\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.7.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 13, 2021, 23:59:00 (Vancouver time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed of randomization and working device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 531\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention! Tidy Submission\n",
    "\n",
    "rubric={mechanics:3}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions).\n",
    "- You should not use this notebook (i.e., Lab3.ipynb) to train your model on Colab. Colab may change the layout of the jupyter notebook, and then we cannot grade on your notebook, the system will miss your grade!\n",
    "- We provide another jupyter notebook (i.e., `Lab3_exp.ipynb`) for you. You can load `Lab3_exp.ipynb` to Colab and run your experiments in this jupyter notebook. \n",
    "- Please download `Lab3_exp.ipynb` from Colab and include it in your final submission. \n",
    "- Some comments in your code will help us grade. Please use heading, comments, and mardown notations to organize your code. \n",
    "- Please feel free to add cells in `Lab3_exp.ipynb`.\n",
    "- You don't need to submit your checkpoints.\n",
    "- You can reuse any scripts of lab tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the questions of this lab, we continue to use the English-French bilingual corpus of [Multi30k](https://github.com/multi30k/dataset) dataset that we used in lab2 and tutorials. Our task is to `translate text from French language to English language`. All your model should be trained on `train_eng_fre.tsv`, validated on `val_eng_fre.tsv`, and tested on `test_eng_fre.tsv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3 Open-NMT translation with beam decoding\n",
    "While it is completely reasonable to use PyTorch to produce models from scratch for Machine Translation, there are some tools and frameworks that can be handy, especially if you don't want to implement tricky aspects of the Machine Translation process, like efficient Beam Decoding. FAIRSeq and OpenNMT are two such frameworks, of which we will look at how to do translation with the latter.\n",
    "\n",
    "Using the provided Open-nmt ipynb in Google Colab and the corresponding config files (multi30k.yml for T3), update the configuration to train an OpenNMT model Encoder-Decoder (w/attention) model for the Multi30k En-Fr dataset. Consider downloading your final checkpoint from colab for safekeeping (you don't need to include this in your submission).\n",
    "\n",
    "OpenNMT_T3.ipynb should be configured to run in Colab and give an overview of how to use OpenNMT as well as sections for the T3 questions.\n",
    "\n",
    "Copy any cells from the Colab notebook that you used to run your model, as well as any updates to the config file that you needed to make (They should be labeled T3.1-T3.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylIL0MBg-NO"
   },
   "source": [
    "\n",
    "## T3.1\n",
    "\n",
    "### Build the vocab for the Multi30k En-Fr dataset\n",
    "rubric={accuracy:2}\n",
    "\n",
    "While just having a vocabulary is fine for some cases, using a sub-word tokenization might help capture morphological information better.\n",
    "\n",
    "\n",
    "To do this, in your config file add ```transforms: [sentencepiece, filtertoolong]``` to both the training and validation corpora.\n",
    "\n",
    "Give the code you ran to build the vocab as well as the \"data\" section of your multi30k config file.\n",
    "\n",
    "Note: Use the train.fr and train-1.en files, or be prepared to split the train_eng_fre.tsv file into two separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93KD6CBVjcQu"
   },
   "outputs": [],
   "source": [
    "# TODO build Multi30k Vocab\n",
    "\n",
    "for file in [\"train.fr\",\"test.fr\",\"val.fr\"]:\n",
    "  with open(file,\"r\", encoding='utf-8') as fp:\n",
    "    outfile = file + \".tokd\"\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "      for line in fp:\n",
    "        tokenized = [tok.text for tok in spacy_fr.tokenizer(line)]\n",
    "        out.write(\" \".join(tokenized)+\"\\n\")\n",
    "for file in [\"train-1.en\",\"test.en\",\"val.en\"]:\n",
    "  with open(file,\"r\", encoding='utf-8') as fp:\n",
    "    outfile = file + \".tokd\"\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "      for line in fp:\n",
    "        tokenized = [tok.text for tok in spacy_en.tokenizer(line)]\n",
    "        out.write(\" \".join(tokenized)+\"\\n\"\n",
    "                  \n",
    "!onmt_build_vocab -config multi30k.yml -n_sample 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsVLrM0Pjfn8"
   },
   "source": [
    "```\n",
    "Include changes you made to the Data saving, Corpus, and Vocab section in the Config HERE\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# multi30k.yaml\n",
    "\n",
    "## TO DO COMPLETE DATA SAVING\n",
    "save_data: multi30k/run/model\n",
    "## Where the vocab(s) will be written\n",
    "src_vocab: multi30k/run/vocab.src\n",
    "tgt_vocab: multi30k/run/vocab.tgt\n",
    "\n",
    "# Corpus opts:\n",
    "data:\n",
    "## TODO COMPLETE CORPUS OPTIONS\n",
    "## Add sentencepiece and filter long segments\n",
    "    corpus_1:\n",
    "        path_src: multi30k/train.fr.tokd\n",
    "        path_tgt: multi30k/train-1.en.tokd\n",
    "        transforms: [filtertoolong]\n",
    "    valid:\n",
    "        path_src: multi30k/val.fr.tokd\n",
    "        path_tgt: multi30k/val.en.tokd\n",
    "        #NOTE NO FILTERTOOLONG --> You don't want to bias your validation score\n",
    "\n",
    "\n",
    "#TODO Fill in vocab you create (already have this above)\n",
    "src_vocab: multi30k/run/vocab.src\n",
    "tgt_vocab: multi30k/run/vocab.tgt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B05G-PaWl0L-"
   },
   "source": [
    "## T3.2\n",
    "### Train Model\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Fill in the multi30k.yaml config to setup a seq2seq model that has a 3 layer RNN encoder 2 layer RNN decoder, MLP attention, with 20% dropout, using Adam as your optimizer.\n",
    "\n",
    "Copy and paste the changed parts of the *.yml file below along with the training command you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWcx3Uuamdil"
   },
   "outputs": [],
   "source": [
    "# TODO Train Model\n",
    "!onmt_train -config multi30k.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBlHa2Wlm4Q9"
   },
   "source": [
    "```\n",
    "Changes to model, and optimizer here.\n",
    "decoder_type: rnn\n",
    "encoder_type: rnn \n",
    "enc_layers: 3\n",
    "dec_layers: 2\n",
    "enc_rnn_size: 500\n",
    "dec_rnn_size: 500\n",
    "dropout: 0.2\n",
    "global_attention : mlp\n",
    "\n",
    "\n",
    "# Optimizer settings\n",
    "optim: adam\n",
    "learning_rate: 0.001  #note changing Adam you should change the starting learning rate\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6WGJkJym-y9"
   },
   "source": [
    "## T3.3\n",
    "### Decoding\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Create predictions for the validation set using your saved models and select the one that has the highest BLEU. You should set beam size to 5 for each of these models.\n",
    "\n",
    "Report the BLEU on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3n0ezhfm9q9"
   },
   "outputs": [],
   "source": [
    "## Code to create predictions and calculate BLEU for models\n",
    "\n",
    "\n",
    "!onmt_translate -model multi30k/run/model_step_3000.pt -src multi30k/val.fr.tokd -output multi30k/val_3000.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\n",
    "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/val_3500.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\n",
    "!onmt_translate -model multi30k/run/model_step_4000.pt -src multi30k/val.fr.tokd -output multi30k/val_4000.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "BLEU = 43.32, 72.5/51.1/37.4/27.4 (BP=0.981, ratio=0.981, hyp_len=13172, ref_len=13426)\n",
    "BLEU = 43.85, 73.1/51.7/38.0/28.1 (BP=0.978, ratio=0.978, hyp_len=13133, ref_len=13426)\n",
    "BLEU = 43.09, 73.3/51.8/37.9/27.9 (BP=0.963, ratio=0.963, hyp_len=12933, ref_len=13426)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcF9qBqknZg0"
   },
   "source": [
    "## T3.4 \n",
    "### Comparing Beam Width\n",
    "rubric={accuracy:3}\n",
    "\n",
    "For your BEST model compare the peformance (Both BLEU and clocktime to run)  with the following Beam Sizes: 5 (done above), 10, 15, and 20.\n",
    "\n",
    "Give your code and outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkYdBODGoJIM"
   },
   "outputs": [],
   "source": [
    "## TODO Beam comparison\n",
    "%%time\n",
    "\n",
    "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam5.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[2021-03-16 01:59:35,899 INFO] Translating shard 0.\n",
    "[2021-03-16 01:59:48,986 INFO] PRED AVG SCORE: -0.3111, PRED PPL: 1.3649\n",
    "CPU times: user 35.7 ms, sys: 12.2 ms, total: 47.9 ms\n",
    "Wall time: 17 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam10.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[2021-03-16 01:56:26,498 INFO] Translating shard 0.\n",
    "[2021-03-16 01:56:49,723 INFO] PRED AVG SCORE: -0.2985, PRED PPL: 1.3478\n",
    "CPU times: user 49.2 ms, sys: 17.4 ms, total: 66.6 ms\n",
    "Wall time: 27.1 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam15.txt -gpu 0 -beam_size 15 -seed 531 -block_ngram 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[2021-03-16 01:57:22,825 INFO] Translating shard 0.\n",
    "[2021-03-16 01:57:57,147 INFO] PRED AVG SCORE: -0.2971, PRED PPL: 1.3459\n",
    "CPU times: user 67.3 ms, sys: 22.3 ms, total: 89.5 ms\n",
    "Wall time: 38.3 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam20.txt -gpu 0 -beam_size 20 -seed 531 -block_ngram 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[2021-03-16 01:58:01,617 INFO] Translating shard 0.\n",
    "[2021-03-16 01:58:45,384 INFO] PRED AVG SCORE: -0.2973, PRED PPL: 1.3462\n",
    "CPU times: user 84.8 ms, sys: 19 ms, total: 104 ms\n",
    "Wall time: 47.6 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation, beam is best at a narrow width (~5) with diminishing returns past that point. Time seems to increase ~linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 OpenNMT Transformer model\n",
    "\n",
    "Also in the assignment repository is a Transformer configuration file: ```config-transformer-base-1GPU.yml``` \n",
    "\n",
    "The next couple questions ask about aspects of this file, with the purpose of getting you familiar with using it in case you want to use a similar configuration for the Lab 4 challenges.\n",
    "\n",
    "### 1.1 Comparison to RNN model\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Ignoring data-processing fields, what fields in the config file seem to be changed from RNN models? Pick two of these changes, look up, and explain what they do (https://opennmt.net/OpenNMT-py/options/train.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here EX:\n",
    "position_encoding: 'true'\n",
    "--> used for applying sinusoidal offsets to each dimension in the input, letting the model know where in the sequence each part of the input data corresponds to, to allow for parallelized translation of timesteps in the transformer model.\n",
    "\n",
    "accum_count: 8\n",
    "--> used to allow larger updates to the gradient to occur by accumulating gradient updates between batches before applying them. In essence allows for a \"larger\" effective batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 Transformer Config\n",
    "rubric={reasoning:1}\n",
    "\n",
    "There are a number of fields set in the config file that are obligatory or strongly recommended for running the Transformer, e.g. Glorot initialization. What are the other fields that must be set or are suggested to be set to run it correctly? (Hint you can look at https://opennmt.net/OpenNMT-py/options/train.html for a better idea of which ones are required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here\n",
    "\n",
    "heads\n",
    "transformer_ff\n",
    "param_init_glorot\n",
    "position_encoding\n",
    "max_relative_positions\n",
    "self_attn_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Size of Transformer\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Try loading the Transformer config file and training with it (you can reuse the data processing sections from either multi30k.yml or toy-ende.yml) but set the number of steps to some trivial amount (e.g. 100). As it starts training it should load, display the model, and give a size for the number of parameters.   \n",
    "EX for RNN:\n",
    "```\n",
    "[2021-03-02 00:42:59,826 INFO] encoder: 16506500\n",
    "[2021-03-02 00:42:59,827 INFO] decoder: 41363820\n",
    "[2021-03-02 00:42:59,827 INFO] * number of parameters: 57870320\n",
    "```\n",
    "Report how many total parameters are required to run the 'default' Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here\n",
    "~90 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have four seq2seq models: (1) basic seq2seq from week2 tutorial (`seq2seq_tutorial.ipynb`); (2) seq2seq with additive attention from week3 tutorial (`attention_tutorial.ipynb`); (3) seq2seq variant in Exercise T2.1; and finally (4) Open-NMT seq2seq with MLP attention. Please use the best models of these five models to answer the following questions. \n",
    "\n",
    "If you had problems creating any of these 4 models, just report your results on the models you were able to make work/develop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via the corpus_bleu() function.\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "```\n",
    "For model (1), my best model obtains 25 cumulative BLEU-4 score with 7 epoch(s).\n",
    "For model (2), my best model obtains 43 cumulative BLEU-4 score with 4 epoch(s).\n",
    "For model (3), my best model obtains 27 cumulative BLEU-4 score with 9 epoch(s).\n",
    "For model (4), my best model obtains 43 cumulative BLEU-4 score with 3500 steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Please evaluate any one of four models on on test set (i.e., `test_eng_fre.tsv`) and report its BLEU-1, cumulative BLEU-2, BLEU-3, and BLEU-4 scores via the corpus_bleu() function.\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hints: \n",
    "- You can use the `inference()` function. But you will need to revise few lines of this function.\n",
    "- You may need to review the BLEU implementation in week 2 tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "```\n",
    "For Model 4 Multi-BLEU we have the following BLEU scores (Individual BLEU 1-4 components respectively):\n",
    "73.1/51.7/38.0/28.1\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "For Model 2 we calculate the results via corpus BLEU (CUMULATIVE BLEU as opposed to individual BLEU component from Multi-BLEU):\n",
    "- **This model obtains 0.72 BLEU-1 score** \n",
    "- **This model obtains 0.60 BLEU-2 score** \n",
    "- **This model obtains 0.51 BLEU-3 score** \n",
    "- **This model obtains 0.43 BLEU-4 score** \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Please explain the results of 2.2 briefly. \n",
    "rubric={reasoning:2}\n",
    "\n",
    "Hints: What is the relationship between BLEU-n scores? Why? \n",
    "\n",
    "BLEU-1> BLEU-2 > BLEU-3 > BLEU-4\n",
    "\n",
    "BLEU measures overlapping windows/sequences between source and target, and is the primary (in many works sole) measure for MT systems. \n",
    "\n",
    "It is harder to match larger windows/sequences exactly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the effects of sentence length, we create two subsets of test file: A. `long_test_eng_fre.tsv` which only includes sentence pairs that English reference is longer than 19 tokens; B. `short_test_eng_fre.tsv` which only includes sentence pairs that English reference is shorter than 9 tokens. `long_test_eng_fre.tsv` includes 67 sentence pairs.  `short_test_eng_fre.tsv` includes 85 sentence pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Please report the cumulative BLEU-4 score on long sentences of test set (i.e., `long_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `long_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains 15 cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains 34 cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains 16 cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains 36 cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Please report the cumulative BLEU-4 score on short sentences of test set (i.e., `short_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `short_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains 31 cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains 43 cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains 31 cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains 44 cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Please compare the results of questions of 2.1-2.3. Which model does perform best? What differences between their performance? How does the length of sentence affect performance?\n",
    "rubric={reasoning:4}\n",
    "\n",
    "Hint:\n",
    "- You can review the Section 5 of [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) to answer this question.\n",
    "- You answer should be less than 100 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "The (4) model performs best. \n",
    "The models with Attention perform better than the models without attention. \n",
    "All the models performance do better on `short_test_eng_fre.tsv` than on the `long_test_eng_fre.tsv` set.\n",
    "Without attention mechnism, the model do poorly on `long_test_eng_fre.tsv` set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 BLEU\n",
    "rubric={reasoning:3}\n",
    "\n",
    "BLEU is one of the most widely used metrics for NLP, but why can't we just use accuracy to measure the success of translations? Does accuracy make sense in machine translation? \n",
    "\n",
    "Two other (previously mentioned) metrics are METEOR and ROUGE. Pick one and briefly summarize how it is different than BLEU and why you might choose to use it instead of BLEU. (CITE SOURCES!)\n",
    "\n",
    "Hint: Feel free to think back to Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "Here is a good writeup of Rouge vs. BLEU https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge\n",
    "\n",
    "With the basic idea that Rouge is better at reflecting recall of the reference, while BLEU is better at measuring precision of correct words in the generated translation.\n",
    "\n",
    "ROUGE is often used in Summarization work as it reflects how often the summary correctly contains important parts.\n",
    "\n",
    "\n",
    "https://thesai.org/Downloads/Volume6No11/Paper_28-Comparative_Study_Between_METEOR_and_BLEU_Methods.pdf\n",
    "\n",
    "For METEOR the big difference is that it uses stems/synonyms to identify unigram overlap. Overall METEOR better reflects human judgement than BLEU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Teacher Forcing\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For training seq2seq models we often use Teacher Forcing as part of the training procedure. Identify two advantages of using this approach in training.  \n",
    "\n",
    "Hint: Feel free to do some research to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "\n",
    "Faster overall to train.\n",
    "\n",
    "Without teacher forcing you might run into convergance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Attention bottleneck\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Learning attention functions automatically require large volumes of data especially for recurrent neural networks. Can you recommend a solution to overcome this problem? Barrett and Bingel investigate an approach to solve this limitation. Take a few minutes to SKIM the abstract and introduction and write a few sentences summarizing takeaways that you can use in practice.\n",
    "\n",
    "Barrett, M., Bingel, J., Hollenstein, N., Rei, M., & SÃ¸gaard, A. (2018, October). Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning (pp. 302-312).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "\n",
    "Using 'human-attention' can be beneficial to NLP tasks, you could potentially use open eyetracking databases (if they are on a relevant corpus) to derive these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Global Attention Mechanisms\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Global attention mechanisms consider all the encoder hidden states when deriving the context vector while local attention mechanisms consider only a subset of the encoder hidden states when deriving the context vector. [Luong et al. 2015](https://arxiv.org/pdf/1508.04025.pdf) proposed **three global attention mechanisms**. In the tutorial, we looked at (terminologies are borrowed from Luongs's paper):\n",
    "* Dot: $e_{ij} = s_{i-1}^Th_j \\in \\mathcal{R}$\n",
    "* Concat: $e_{ij} = v_a tanh( W_a [s_{i-1};h_j;]) \\in \\mathcal{R}$ where $W_a \\in \\mathcal{R}^{h\\times 2h}$ and $v_a \\in \\mathcal{R}^{1\\times h}$\n",
    "\n",
    "Can you find out the third global attention mechanism by going over the paper (especially Section 3.1) and write it down in the same format as above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "* General: $e_{ij} = s_{i-1}^TW_ah_j$ Which is basically just Dot product attention with a learned Weight matrix to modify the importance of relationships between different parts of a sentence. Dot attention is General attention with  $W_a = I$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
