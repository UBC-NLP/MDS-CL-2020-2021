# COLX 585 Trends in Computational Linguistics

## Introduction                        

This goal of this course is two-fold. First, the course aims at introducing ``recent deep learning of NLP methods``. Second, the course will function as a context for ``further enhancing student knowledge of deep learing software engineering`` in a project-based fashion where students work on a problem of their choice for the whole course. 

__Slack Channel__: cl-585_trends


## Learning Outcomes
Upon completion of this course students will be able to:

* *identify, motivate, and propose* a research plan for an important, meaningful, and possibly socially relevant problem
* *become* aware of the major engineering and research efforts related to the chosen project and major approaches to solve it
* *apply* deep learning of NLP methods to provide solutions on the problem of choice
* *present*, clearly, their proposed solution in the form of an oral presentation, a written report, and code
* *develop* effective strategies for teamwork

---
# Class Meetings

* This course occurs during Block 6 in the in the 2020/21 school year.

### Lectures: 

* Lectures will be delivered online real-time on Tuesdays and Thursdays, 10:30-12:00 PT, [@Zoom](https://ubc.zoom.us/j/62467194367?pwd=VEgveVZBNENncXo1R0lhUG03RHBUUT09) (pwd: 4242)   
* Some pre-recorded lectures will be provided beforehand for students to watch. 

### Labs: 
* Labs will be delivered online real-time on Thursday, 14:00 â€“ 16:00 PT via Zoom.
* Zoom Link for real-time lectures: (Check Slack).

---
## Project deadlines

This is an __project-based course__. You will work in assigned groups of three or four. You'll be evaluated as follows:

| Assessment       | Weight  | Deadline        | Location |
|------------------|---------|------------------|----------|
| Milestone 1: Project Proposal | 10%     | April 3, 12:59pm | On Github |
| Milestone 2: Progress Report 1 | 15%     |  April 10, 12:59pm | On Github |
| Milestone 3: Progress Report 2 | 15%     | April 17, 12:59pm | On Github |
| Milestone 4_a: Project Presentation | 5%   | April 21 TBD    | On [Zoom](https://ubc.zoom.us/j/62467194367?pwd=VEgveVZBNENncXo1R0lhUG03RHBUUT09)  |
| Milestone 4_b: Final Project | 25%   | April 25, 12:59pm    | On Github |
| Teamwork |  30% | April 28, 12:59pm  | On Github |

Note that for all but Teamwork, all students in the same group will receive the same grade. Students who get a failing grade on teamwork can get no better than a C in the course.

---
## Teaching Team

| Position           | Name    | Slack Handle | GHE Handle |
| :----------------: | :-----: | :----------: | :--------: |
| Main Instructor | Muhammad Abdul Mageed |    `@Muhammad Mageed`       | `@amuham01`        |
| Teaching Assistant | Peter Sullivan |    `@prsull`       | `@prsull`        |
| GRA | Chiyu Zhang | `@Chiyu Zhang` | `@chiyuzh` |
| Lab Instructor | Ganesh Jawahar | `@ganeshjw` | `@ganeshjw` |

---
## Office Hours: 
**Muhammad:**    Wed. 12:00-14:00 PT  [@Zoom](https://ubc.zoom.us/j/62467194367?pwd=VEgveVZBNENncXo1R0lhUG03RHBUUT09) 

**Peter:** Fri. 11:00-12:00 PT  (Check Slack for Zoom Link) 

**Chiyu:** Thurs. 14:00-15:00 on Slack

**Ganesh:**  Thurs. 18:00-19:00 via [Zoom](https://ubc.zoom.us/j/67010634869?pwd=RFVZVWxpU3lsZGtmMU9vWXUwWHBBdz09)

---
# Lecture Details (Tentative)
| Lecture | Topic   | Readings                 | Lecture |    Notes |
|------   | ------- |--------------------------| -------- | -------- |
| 1. (Tues, March 30)  | Course Overview | NA | [[slides](lectures/xxx)]; [Video: ELMo](https://www.youtube.com/watch?v=3qhriESuX9Y&t=2497s); [[Lec_video](xxx)]; ```pwd:```    |  NA |
| 2. (Thu, April 1)  | Contextual Word Embeddings | [Semi-supervised sequence tagging with bidirectional language models](https://www.aclweb.org/anthology/P17-1161.pdf), [ELMo](https://www.cs.ubc.ca/~amuham01/LING530/papers/petersELMo2018.pdf) | [[slides](lectures/xxx)]; [Video: ELMo](https://www.youtube.com/watch?v=3qhriESuX9Y&t=2497s); [[Lec_video](xxx)]; ```pwd:```    |  NA |
| 3. (Tues, April 6)  | ConvNets | [DLB-CH09](https://www.deeplearningbook.org/contents/convnets.html) | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    |  NA |
| 4. (Thu, April 8)  | RNN Language Models & Project Q & A | NA | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    | NA |
| 5. (Tues, April 13)  | Self-Supervised Learning (BERT) | [BERT](https://arxiv.org/pdf/1810.04805.pdf); ```extra:``` [Roberta](https://arxiv.org/pdf/1907.11692.pdf), [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), [SpanBERT](https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00300), [MASS](https://arxiv.org/pdf/1905.02450.pdf) | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    |  NA |
| 6. (Thu, April 15)  |  Generative Pre-Training (GPT) | [Improving Language Understanding by Generative Pre-Training(GPT-1)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [Language Models are Unsupervised Multitask Learners(GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    |  NA |
| 7. (Tues, April 20)  | Paraphrase Models & Style Transfer | NA | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    |  NA |
| 8. (Thu, April 22)  | Project Presentations | NA | [[slides](lectures/xxx)]; [[video](xxx)]; ```pwd:```    |  NA |
---


# Tutorials 
 
In Computational Linguistics it's expected to have a broad familiarity with the varied tasks and topics of the field, even though you may not be working on a specific topic in your day-to-day life. As a trends course, we thus aim to provide tutorials on a number of different techniques and topics, that may be useful to familiarize yourself with, even if you may not need them in your specific project. Our goal is to allow you to primarily investigate tools and techniques which will be useful to your project, while also secondarily allowing you to familiarize yourself with other techniques/tasks/tools you may encounter in the field.  

To restate: because people may be picking quite different projects, some of these tutorials may be more relevant to your group's project than others, and thus you will work with the TA team to identify your specific "need-to-knows" relevant to your project and which tutorials (or other resources) might meet these knowledge gaps. It is *encouraged*, but *not required* to watch all the videos/ do all the tutorials (and in fact may not be feasible to master all these techniques).  

(Note this will update as we add more tutorials/topics relevant to people's projects)

| Topic w/ Video Links   | Notebook Link  |
|----- | --- |
| [BERT](https://youtu.be/8fPh2wiPPy4) (BERT pre-trained transformer models) | |
| [BPE](https://youtu.be/sOT9IHN_iwk) (BytePairEncoding) | |
| CNN ([image](https://youtu.be/32txg97UGI4) and [text](https://youtu.be/yBqfOknzOCk)) | |
| [Fairseq](https://youtu.be/6G6jFc9uo0c) (Facebook AI Research's Seq2Seq toolkit) | | 
| More to come! | |

# Additional Papers & Resources


#### Sub-Word Representations (Byte-pair Encoding)
* [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162.pdf)
* [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf)
* [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf)
* [A New Algorithm for Data Compression](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)

## Various Generration Papers

* [Neural Text Degeneration with Unlikelihood Training](https://arxiv.org/pdf/1908.04319.pdf)
* [Defending Against Neural Fake News(GROVER)](https://arxiv.org/pdf/1905.12616.pdf)
* [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/pdf/1909.05858.pdf)
* [Plug and Play Language Models: A Simple Approach to Controlled Text Generation(PPLM)](https://arxiv.org/pdf/1912.02164.pdf)
* [The Curious Case of Neural Text Degeneration(nucleus samples)](https://arxiv.org/pdf/1904.09751.pdf)


## Style Transfer: 
* [Multiple-Attribute Text Style Transfer](https://arxiv.org/pdf/1811.00552.pdf)
* [Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer](https://arxiv.org/pdf/1803.06535.pdf)
* [Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation Comprehension](https://papers.nips.cc/paper/9284-controllable-unsupervised-text-attribute-transfer-via-editing-entangled-latent-representation.pdf)

## Python Tools
* [Transformers](https://huggingface.co/transformers/)
* [Sentencepiece](https://github.com/google/sentencepiece)
* [BERT tokenizer](https://github.com/google-research/bert/blob/master/tokenization.py)

---
### Where to find good papers:
* [[ACL Anthology](https://www.aclweb.org/anthology/)]
* [[NeurIPS 2020](https://papers.nips.cc/paper/2020)]; [[NeurIPS 2019](https://nips.cc/Conferences/2019/AcceptedPapersInitial)]; [[NeurIPS 2018](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)]
* [[ICLR 2020](https://iclr.cc/virtual_2020/papers.html?filter=keywords)]; [[ICLR 2019](https://iclr.cc/Conferences/2019/Schedule?type=Poster)]; [[ICLR 2018](https://dblp.org/db/conf/iclr/iclr2018)]
* [[AAAI Digital Library](https://www.aaai.org/Library/conferences-library.php)]
* [[ACL Anthology](https://www.aclweb.org/anthology/)]

# Policies

* Please see the general [MDS policies](https://ubc-mds.github.io/policies/).
* Note: 5% of an assignment will be lost for each 24 hour period up to the maximum of total assignment weight.
* Assignments submitted by 12/noon the next day of due date will not be penalized. Delay after 12/noon of the next day of due date will be penalized by 5% for that day. That is, automatic externsion does not apply after 12/noon of the next day of assignment due date.   
