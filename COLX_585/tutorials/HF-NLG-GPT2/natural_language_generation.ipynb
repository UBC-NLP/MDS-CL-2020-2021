{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "natural_language_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHqeMTNSZxEj"
      },
      "source": [
        "# COLX 585 Trends in Computational Linguistics\n",
        "## Generating Natural Language Text using GPT-2\n",
        "\n",
        "### Goal of this tutorial:\n",
        "- Know the background of GPT-2, a Language Model (LM)\n",
        "- Learn how to train GPT-2 (toy version) from scratch on Yelp dataset\n",
        "- Learn about deterministic decoding techniques like greedy search and beam search.\n",
        "- Learn aboout stochastic decoding techniques like top-k and top-p (nucleus) sampling\n",
        "\n",
        "###  General:\n",
        "- This notebook was last tested on Python 3.6.4, PyTorch 0.4.0, **transformers 2.1.1** (note: tutorial might work only with this version of transformers)\n",
        "- We would like to acknowledge the transformers repository from huggingface (https://github.com/huggingface/transformers) which we used as a reference to code up both GPT-2 and decoding techniques\n",
        "- This tutorial gives cuda out of memory error when run in Colab. Hence, it is recommended to run it on a local machine (CPU) with jupyter notebook\n",
        "\n",
        "### References\n",
        "To know more about the above-mentioned concepts, take a look at the following articles:\n",
        "1. GPT-1 Original Paper https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "2. GPT-2 Original Paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "3. Neural Text Degeneration with Unlikelihood Training https://arxiv.org/pdf/1908.04319.pdf (see Section 3)\n",
        "4. The Curious Case of Neural Text Degeneration https://arxiv.org/abs/1904.09751"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkWMw0UgdoR2"
      },
      "source": [
        "## Recap of last week\n",
        "\n",
        "### Bidirectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "Last week, we saw BERT model, which is used to represent a piece of text (e.g., sentence, sentence pair, document) to solve a wide variety of natural language understanding tasks (e.g., question answering, natural language inference, text classification). At its core, BERT is based on **transformer** architecture to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. The pretraining objectives underlying BERT model are masked language modeling and next sentence prediction. BERT's model architecture is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1PXjMq2DEXFSsY1AsYhKIfUSreLxHtPR4\" height=\"250\"/>\n",
        "\n",
        "Picture courtesy: https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUb589_xZxEk"
      },
      "source": [
        "## Background\n",
        "\n",
        "\n",
        "### Generative Pre-Training - Version 2 (GPT-2)\n",
        "\n",
        "GPT-2 model is the state of the art model for natural language generation (NLG). GPT-2 model is based on **Transformer architecture** and uses language modeling (LM) objective, which works by maximizing the conditional probability of predicting a word in a text sequence (e.g., sentences) given the previous words of the target word. The model architecture is as shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12dvGzUzM_hM_LTAT8XfmJmsL8cu1Jyx3\" alt=\"GPT-2 model architecture\" title=\"GPT-2 model\" height=300 />\n",
        "\n",
        "Unlike BERT model that learns **bidirectional** representation, GPT-2 learns **unidirectional** representation (left to right) by conditioning on only left context in all layers. While BERT model is used primarily for solving **natural language understanding** tasks, GPT-2 model is primarily used for **natural language generation** (see next section).\n",
        "\n",
        "GPT-2 is trained on **WebText** that contains huge amounts of web pages. In this tutorial, we will stick to training GPT-2 on **Yelp dataset** which doesn't require much computing resources in comparison with training on WebText.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvHmRGqkgnAs"
      },
      "source": [
        "### Natural Language Generation\n",
        "Once we train a GPT-2 model, we might be keen to generate text by prompting the model with a seed text (e.g., beginning of a news article). One of the most popular generation from GPT-2 is:\n",
        "\n",
        "<img src=\"https://pbs.twimg.com/media/DzYpsJOU0AA1PO9.png\" alt=\"Famous unicorn example from GPT-2 Generations\" title=\"GPT2 - Generation\" height=600 width=550 />\n",
        "\n",
        "As you can observe, the resulting text seems to be **mostly coherent, grammatical, using long-term context and world knowledge**. We generally rely on either deterministic or stochastic decoding techniques to generate the text given a seed text and a trained language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iiG4ZWJgnup"
      },
      "source": [
        "### Deterministic Decoding Techniques\n",
        "\n",
        "There are two widely used decoding techniques, **greedy search** and **beam search**. The former is a special case of the latter. \n",
        "\n",
        "### Greedy search\n",
        "\n",
        "**Greedy search**, as the name suggests, outputs the token that received the highest probability at each time step. We have already used greedy search to generate translation from the decoder of the neural machine translation model. \n",
        "\n",
        "Let us see a sample generation using greedy search. During decoding, we first feed the seed text (prompt) as input to the model and predict the next most likely (argmax) word (first word in generation which is `the` in this example).\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1QBKIlCj3y8kZ27x5ojXcKANsOXga2qGf\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />\n",
        "\n",
        "Once we generate the first word, we append this word (`the`) to the input and predict the next most likely (argmax) word (second word in generation which is `scientist` in this example).\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=12eTq-gnjB9MM7BZb3IfXmWzBSM1LaXTL\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />\n",
        "\n",
        "We typically repeat the process until we have generated a certain number of words.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1feSME5YWr36MpZbfyWX0G1mdlXsv5j7G\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=370 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad6KqM2csZTO"
      },
      "source": [
        "### Beam Search \n",
        "**On the other hand, beam search** maintains a fixed-size (which we call beam size) set of partially-decoded sequences, called hypotheses. At each time step, beam search creates new hypotheses by appending each token in the vocabulary to each existing hypothesis, scoring the resulting sequences. \n",
        "\n",
        "Let us see an example for beam search (with beam size of 2). In the first timestep, we feed the input (say `the unicorns spoke perfect english`) to the model, predict the top 2 words (based on the model probability for a word given the input) and store them in beam.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1gPCwzOUZOHbKl2gpHPtATr2YpLHUL-v_\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
        "\n",
        "**In the second timestep**, for each word in the beam, we append the word in the original input, feed to the model, keep track of all the resulting sequences (input along with generated word) along with their probabilites. We pick the top 2 resulting sequences and store them in beam.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1xuCidto4yjCBawiz9STcrUZ4UAVyANgr\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
        "\n",
        "**In the third timestep**, for each sequence in the beam, we append the sequence in the original input, feed to the model, keep track of all the resulting sequences (input along with generated word) along with their probabilites. We pick the top 2 resulting sequences and store them in beam. We repeat the process until we have generated the required number of words. The sequence in the beam with the highest probability is the generated sequence.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1hP8lfxkvVDN9q3Bu1WKWfKqjJMrHMVMo\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
        "\n",
        "In an open-domain generation, beam search generally leads to **degenerate text** with lot of repetitions (as seen in the below figure) compared to the admirable quality of the text decoded by stochastic method (see next section) like top-k sampling.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1Zcmch-8vxIhAImn7RKJObzz0DSw5tabi\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n",
        "\n",
        "Picture courtesy: https://arxiv.org/pdf/1904.09751.pdf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoidIkjYgn4L"
      },
      "source": [
        "\n",
        "### Stochastic Decoding Techniques\n",
        "Stochastic decoding techniques sample from a model-dependent distribution at each step. The two successful techniques in this category are **top-k and nucleus (top-p)** sampling. To prevent sampling low probability tokens, a typical approach is to restrict sampling to a subset of the vocabulary at each step. \n",
        "\n",
        "The top-k sampler restricts sampling to the k most-probable tokens as shown below in an example.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1rjd8FjS_r1MmXwn8Hnbms7NgJJTa3x2j\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n",
        "\n",
        "\n",
        "Instead, **the nucleus (top-p) sampler** restricts sampling to the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1) as shown below in an example.\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1P3n2ygqxEQsYre-i_CvWQAOsqlzdho4Z\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhe-DEByyDf6"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "### GPT-2 Pre-training \n",
        "\n",
        "We will now perform the following:\n",
        "- Create a toyish GPT-2 configuration by modifying the original configuration.\n",
        "- Train a GPT-2 language model (using toyish hyperparameter configuration) from scratch on Yelp dataset\n",
        "- Load the original GPT-2 language model (pretrained by the original authors)\n",
        "- Prompt the model with seed text and decode using \n",
        " - Greedy search\n",
        " - Top-k sampling\n",
        " - Top-p (or nucleus) sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhlL1NSzdoR_"
      },
      "source": [
        "### Prepare the GPT-2 training samples\n",
        "* Please download Yelp dataset from https://www.kaggle.com/omkarsabnis/yelp-reviews-dataset\n",
        "* the file we're going to use: **yelp.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Stf3dsAdoSA"
      },
      "source": [
        "As usual, let us start by loading the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU6BjQ8tif6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088e7fd3-2e21-42b1-8977-4617fb040374"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxd1-CiylpU"
      },
      "source": [
        "Install the transformers library (contains GPT-2 model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQRlA7hvzDus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fce9e16-7d9c-4136-9d26-b99f68b2871f"
      },
      "source": [
        "!pip install transformers==2.1.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.1.1 in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (0.1.95)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (1.17.41)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.1.1) (0.3.6)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.1.1) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.41 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.1.1) (1.20.41)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.41->boto3->transformers==2.1.1) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur-FXgpwzQpv"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe5h1kdcZxEk"
      },
      "source": [
        "'''\n",
        "One place for all the imports\n",
        "'''\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
        "\n",
        "# import GPT-2 specific classes\n",
        "from transformers import (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AdamW, WarmupLinearSchedule)\n",
        "\n",
        "import logging\n",
        "logging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)\n",
        "\n",
        "# set the seed\n",
        "manual_seed = 123\n",
        "random.seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "  torch.cuda.manual_seed(manual_seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD532G9i0GSg"
      },
      "source": [
        "Load GPT-2 config from library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJ2shXpZxEp"
      },
      "source": [
        "cache_dir = \"/tmp\" # to store pretrained checkpoints\n",
        "\n",
        "# load config\n",
        "original_config = GPT2Config.from_pretrained(\"gpt2\", cache_dir=cache_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Y0n_QZ1Ygs"
      },
      "source": [
        "Modify the config (hyperparameters) to train a toyish GPT-2 based language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAGKyv4K1Yq-"
      },
      "source": [
        "# construct a tutorial configuration file to setup a small version of GPT-2\n",
        "config = original_config\n",
        "config.n_layer = 2 # Number of hidden layers in the Transformer encoder. (default 12)\n",
        "config.n_embd = 60 # Dimensionality of the embeddings and hidden states. (default 768)\n",
        "config.n_head = 2 # Number of attention heads for each attention layer in the Transformer encoder. (default 12)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMBmc1w1mdD"
      },
      "source": [
        "Instantiate the toyish GPT-2 model and GPT-2 tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gHlWNTw1m3w"
      },
      "source": [
        "# load model\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False, cache_dir=cache_dir)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72VN9BPV17DN"
      },
      "source": [
        "Prepare the Yelp dataset used to train our LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW1qHG5sZxEw",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17aa3aa4-494a-41da-c5b9-93e96cd39dba"
      },
      "source": [
        "'''\n",
        "prepare the GPT-2 training samples\n",
        "* download Yelp dataset from https://www.kaggle.com/omkarsabnis/yelp-reviews-dataset\n",
        "* the file we're going to use: yelp.csv\n",
        "'''\n",
        "import csv\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, tokenizer, file_path, block_size=512):\n",
        "    assert os.path.isfile(file_path)\n",
        "    print('creating features from dataset: %s'%file_path.split('/')[-1])\n",
        "    # read raw text from the csv file.\n",
        "    raw_text = ''\n",
        "    csv_reader = csv.DictReader(open(file_path))\n",
        "    li = 0 # line index\n",
        "    for row in csv_reader:\n",
        "      raw_text = raw_text + '%s '%row[\"text\"]\n",
        "      li = li + 1\n",
        "      if li > 300: # breaking after reading 300th line.\n",
        "        break\n",
        "\n",
        "    # tokenize raw text\n",
        "    tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(raw_text))\n",
        "    # create segments\n",
        "    self.examples = []\n",
        "    for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
        "      self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    return torch.tensor(self.examples[item])\n",
        "    \n",
        "# path of yelp.csv file \n",
        "train_file = \"yelp.csv\" \n",
        "block_size = 64\n",
        "\n",
        "train_dataset = TextDataset(tokenizer, train_file, block_size=block_size)  \n",
        "print(\"%d instances of block size %d created\"%(len(train_dataset), block_size))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating features from dataset: yelp.csv\n",
            "760 instances of block size 64 created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFd4V274PRC"
      },
      "source": [
        "Print a sample batch (of size 2) from the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4lrIaC4dWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08c7ee9-a21f-469a-a27d-22474296962f"
      },
      "source": [
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=2)\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch.size())\n",
        "  break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2D-CSbv3eP0"
      },
      "source": [
        "Hyperparameters of our toyish GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFekzEpzZxEz"
      },
      "source": [
        "train_batch_size = 50 # batch size \n",
        "max_steps = 25 # maximum training steps\n",
        "learning_rate = 5e-5 # the initial learning rate for Adam.\n",
        "adam_epsilon = 1e-8 # epsilon for Adam optimizer.\n",
        "warmup_steps = 0 # linear warmup over warmup_steps.\n",
        "num_train_epochs = 1 # total number of training epochs to perform.\n",
        "max_grad_norm = 1.0 # max. gradient norm.\n",
        "weight_decay = 0.0 # weight deay if we apply some."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBUu4l2W73WZ"
      },
      "source": [
        "Iterator for creating batches from training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otYq6pCx77LI"
      },
      "source": [
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
        "num_train_epochs = max_steps // (len(train_dataloader)) + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFZFrfDD7-IU"
      },
      "source": [
        "Prepare optimizer and schedule (linear warmup and decay)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lmfiRaCZxE2"
      },
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=max_steps)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOMiCkxK8HGJ"
      },
      "source": [
        "Kick-start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs5BQKv28HWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01711d6-5c46-42e4-c216-6dd0096650ff"
      },
      "source": [
        "# bookkeeping variables\n",
        "global_step = 0\n",
        "tr_loss = 0.0\n",
        "cur_epoch = 0\n",
        "\n",
        "model.zero_grad() # clears the gradient buffer\n",
        "train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=False)\n",
        "\n",
        "# run through the dataset\n",
        "model.train() # set the mode as training\n",
        "for _ in train_iterator:\n",
        "  epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "    # read a batch\n",
        "    inputs, labels = (batch, batch)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # pass the data to the model\n",
        "    outputs = model(inputs, labels=labels)  # note that labels are shifted inside the model (reference: https://huggingface.co/transformers/model_doc/gpt2.html)\n",
        "    \n",
        "    # seek the loss\n",
        "    loss = outputs[0] \n",
        "    tr_loss += loss.item()\n",
        "    \n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    \n",
        "    # update model parameters\n",
        "    optimizer.step()  # update the model parameter\n",
        "    scheduler.step()  # update learning rate schedule\n",
        "    model.zero_grad() # clear the gradient buffer\n",
        "    \n",
        "    global_step += 1\n",
        "\n",
        "    if max_steps > 0 and global_step > max_steps:\n",
        "      epoch_iterator.close()\n",
        "      break\n",
        "\n",
        "  if max_steps > 0 and global_step > max_steps:\n",
        "    train_iterator.close()\n",
        "    break\n",
        "    \n",
        "  cur_epoch += 1\n",
        "  print(\"%d epoch loss: %.2f\"%(cur_epoch, tr_loss/global_step))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:  12%|█▎        | 2/16 [00:00<00:00, 15.36it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 4/16 [00:00<00:00, 15.65it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 6/16 [00:00<00:00, 16.26it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 8/16 [00:00<00:00, 16.50it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 10/16 [00:00<00:00, 16.60it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 12/16 [00:00<00:00, 16.95it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 16/16 [00:00<00:00, 17.48it/s]\n",
            "Epoch:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]\n",
            "Iteration:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 2/16 [00:00<00:00, 19.18it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 epoch loss: 10.80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  25%|██▌       | 4/16 [00:00<00:00, 18.86it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 6/16 [00:00<00:00, 18.47it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 8/16 [00:00<00:00, 14.54it/s]\n",
            "Epoch:  50%|█████     | 1/2 [00:01<00:01,  1.49s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPl5aBSZxE8"
      },
      "source": [
        "## Text Generation with GPT-2 \n",
        "Now that we got a glimpse of training GPT-2 (toy version) from scratch, we can move to generating some interesting text from GPT-2 given some seed text.\n",
        "\n",
        "Let us create some seed text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v64O1YJvZxE-"
      },
      "source": [
        "'''\n",
        "define some seed text to be given as input to GPT-2 model\n",
        "'''\n",
        "\n",
        "SEED_TEXT = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
        "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
        "researchers was the fact that the unicorns spoke perfect English. The\"\"\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgZMlNGnZxFG"
      },
      "source": [
        "Since our GPT-2 model trained on Yelp is toyish, we will use the original GPT-2 model pretrained on WebText for the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mt_CigxZxFG"
      },
      "source": [
        "'''\n",
        "load model and tokenizer for GPT-2 (pretrained version)\n",
        "'''\n",
        "cache_dir = \"/tmp\" # to store pretrained checkpoints\n",
        "\n",
        "# load model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir=cache_dir)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False, cache_dir=cache_dir)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ-rsh-28kOb"
      },
      "source": [
        "Set the number of words to generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7SayHdJZxFJ"
      },
      "source": [
        "'''\n",
        "parameter for generation\n",
        "'''\n",
        "num_words_to_generate = 70"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDC4gpTyZxFP"
      },
      "source": [
        "### Greedy Search\n",
        "\n",
        "Let us start with the most simplest but not very effective decoding technique, greedy search. Greedy search outputs the token that received the highest probability at each time step. \n",
        "\n",
        "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6zNZTMWePFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1637467e-87a9-4113-bb31-0d41dd62669d"
      },
      "source": [
        "# tokenize all the words in seed text using GPT-2 tokenizer\n",
        "seed_tokens = tokenizer.encode(SEED_TEXT, add_special_tokens=False)\n",
        "print(len(seed_tokens)) # there are 53 GPT-2 tokens in the seed text\n",
        "print(seed_tokens[0:10]) # token ids for first 10 tokens\n",
        "print(tokenizer.decode(seed_tokens[0:10])) # raw text based on first 10 tokens"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n",
            "[818, 257, 14702, 4917, 11, 11444, 5071, 257, 27638, 286]\n",
            "In a shocking finding, scientist discovered a herd of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVruSHJFfZIT"
      },
      "source": [
        "We will convert the token ids to tensor so that we can feed it to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJlmUhV0gGYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6761551-7103-4dce-adc1-1a0757684c69"
      },
      "source": [
        " # create the tensor to store the model input (initially the input is just the seed text)\n",
        "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(generated)\n",
        "print(generated.size())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "torch.Size([1, 53])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMBaa_DggbkJ"
      },
      "source": [
        "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Z8WzjIgupg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3663182c-f1b3-4076-9175-356b20b2dac5"
      },
      "source": [
        "# prepare gpt-2 model input\n",
        "inputs = {'input_ids': generated}\n",
        "\n",
        "# feed input to the model\n",
        "outputs = model(**inputs)[0]\n",
        "\n",
        "print(outputs.size()) # [batch size, number of tokens in input, number of tokens in GPT-2 vocabulary]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 53, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJVnztvvm8sP"
      },
      "source": [
        "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFeiceN_ocCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fcc90d-de98-4684-ac59-cb93639fc16e"
      },
      "source": [
        "# extract the next token logits (unnormalized probability distribution)\n",
        "next_token_logits = outputs[:, -1, :]\n",
        "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBdt2g7aocgK"
      },
      "source": [
        "**In greedy sampling, we choose the word with the highest probability as the word to be generated.**\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1QBKIlCj3y8kZ27x5ojXcKANsOXga2qGf\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z2cxOfun9xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e43bd7-195e-4c0e-aaaa-58c95e34c9ae"
      },
      "source": [
        "# find the token with the highest probability (argmax)\n",
        "next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "print(next_token) # id for the predicted token"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4837]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI_Vby5-pQqD"
      },
      "source": [
        "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjakss0iqgf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cde4360-6290-4a51-deaf-731c7a8b205c"
      },
      "source": [
        "print('before appending the new generated token...')\n",
        "print(generated)\n",
        "# add the generated token to the input\n",
        "generated = torch.cat((generated, next_token), dim=1)\n",
        "print('after appending the new generated token...')\n",
        "print(generated)\n",
        "print('our text (seed text + generated text) so far...')\n",
        "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "after appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383,  4837]], device='cuda:0')\n",
            "our text (seed text + generated text) so far...\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The researchers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD9MaGmDsXnT"
      },
      "source": [
        "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
        "\n",
        "And the full fledged code for greedy search will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO6hMB1BZxFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e44e9bf-fcc0-4168-81c3-6dcd94965e85"
      },
      "source": [
        "def greedy_search(seed_text):  \n",
        "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
        "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
        "  \n",
        "  # create the tensor to store the model input (initially the input is just the seed text)\n",
        "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for _ in range(num_words_to_generate): # run over number of word to generate\n",
        "      # prepare gpt-2 model input\n",
        "      inputs = {'input_ids': generated}\n",
        "\n",
        "      # feed input to the model\n",
        "      outputs = model(**inputs)[0]\n",
        "\n",
        "      # extract the next token logits (unormalized probability distribution)\n",
        "      next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "      # find the token with highest probability (argmax)\n",
        "      next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "      # add the generated token to the input\n",
        "      generated = torch.cat((generated, next_token), dim=1)\n",
        "  \n",
        "  # convert the model generation from token ids to raw text\n",
        "  generated = generated[:, len(seed_tokens):].tolist() # seed tokens are already in raw form\n",
        "  for g in generated: # for every generated token\n",
        "    text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  return text\n",
        "\n",
        "print(\"greedy search's seed text = %s\"%SEED_TEXT)\n",
        "print(\"greedy search's continuation text: %s\"%greedy_search(SEED_TEXT))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "greedy search's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The\n",
            "greedy search's continuation text:  researchers say that the unicorns were able to communicate with each other through their\n",
            "\n",
            "speech, and that they were able to communicate with each other through their eyes.\n",
            "\n",
            "The researchers say that the unicorns were able to communicate with each other through their eyes.\n",
            "\n",
            "The researchers say that the unicorns were able to communicate with each other through\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kZQRI05ZxFV"
      },
      "source": [
        "Greedy search clearly suffers from repetition problem. In general, greedy search doesn't work well for open-domain generation.\n",
        "\n",
        "To overcome the repetition problem and also encourage diverse text, we will be exploring the stochastic methods that works by preventing sampling of low probability tokens. A typical approach is to restrict sampling to a subset of the vocabulary at each step.\n",
        "\n",
        "### Top-k sampling\n",
        "The top-k sampler restricts sampling to the k most-probable tokens. This was the decoding strategy used by the original GPT-2 paper.\n",
        "\n",
        "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCTey_hftTDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6bbe66-a7c7-4c8b-8d46-8183afb03bab"
      },
      "source": [
        "# tokenize all the words in seed text using GPT-2 tokenizer\n",
        "seed_tokens = tokenizer.encode(SEED_TEXT, add_special_tokens=False)\n",
        "print(len(seed_tokens)) # there are 53 GPT-2 tokens in the seed text\n",
        "print(seed_tokens[0:10]) # token ids for first 10 tokens\n",
        "print(tokenizer.decode(seed_tokens[0:10])) # raw text based on first 10 tokens"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n",
            "[818, 257, 14702, 4917, 11, 11444, 5071, 257, 27638, 286]\n",
            "In a shocking finding, scientist discovered a herd of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbTz7Ag9tV1q"
      },
      "source": [
        "We will convert the token ids to tensor so that we can feed it to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPBNxqBetbKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fc4bbc-bcce-4e5c-eb2a-7ec05b7d36da"
      },
      "source": [
        "# create the tensor to store the model input (initially the input is just the seed text)\n",
        "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(generated)\n",
        "print(generated.size())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "torch.Size([1, 53])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U_x-TMdtehf"
      },
      "source": [
        "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YBa3yeDtgPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4114f0-76af-4634-8f78-c1a258f26e94"
      },
      "source": [
        "# prepare gpt-2 model input\n",
        "inputs = {'input_ids': generated}\n",
        "\n",
        "# feed input to the model\n",
        "outputs = model(**inputs)[0]\n",
        "\n",
        "print(outputs.size()) # batch size x number of tokens in input x number of tokens in GPT-2 vocabulary"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 53, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhtJihUTtkQb"
      },
      "source": [
        "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBgdORtxto1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9026672b-3f70-4cb2-f6e8-1b5954b74704"
      },
      "source": [
        "# extract the next token logits (unnormalized probability distribution)\n",
        "next_token_logits = outputs[:, -1, :]\n",
        "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krRPfV-4uHpl"
      },
      "source": [
        "**In top-k sampling, we sample the word from the k most probable tokens to generate the next word.**\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1rjd8FjS_r1MmXwn8Hnbms7NgJJTa3x2j\" alt=\"Top-k Sampling\" title=\"Top-k Sampling\"  height=270 />\n",
        "\n",
        "So we first remove all tokens with a probability less than the last token of the top-k list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlYfE20svL3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d51013f-f433-4577-b83c-0fe4aada402f"
      },
      "source": [
        "# Remove all tokens with a probability less than the last token of the top-k list\n",
        "indices_to_remove = next_token_logits < torch.topk(next_token_logits, 40)[0][..., -1, None] # top_k = 40\n",
        "next_token_logits[indices_to_remove] = -float('Inf') # substitute negative infinity so that those words would never be sampled\n",
        "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dba24mOdvgjV"
      },
      "source": [
        "Now we can sample from top-k probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKfDJctivlMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baccc627-135a-4814-beb5-a5522dda5694"
      },
      "source": [
        "# Sample from top-k probability distribution\n",
        "next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "print(next_token) # id for the predicted token"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1074]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PetbmddmwKt4"
      },
      "source": [
        "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JztuW4C-wL8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd3444b-22dc-4d10-e7f4-afc7ca4b7145"
      },
      "source": [
        "print('before appending the new generated token...')\n",
        "print(generated)\n",
        "# add the generated token to the input\n",
        "generated = torch.cat((generated, next_token), dim=1)\n",
        "print('after appending the new generated token...')\n",
        "print(generated)\n",
        "print('our text (seed text + generated text) so far...')\n",
        "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "after appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383,  1074]], device='cuda:0')\n",
            "our text (seed text + generated text) so far...\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The team\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evExzQGZxSDg"
      },
      "source": [
        "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
        "\n",
        "And the full fledged code for top-k sampling will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJsOzHtwZxFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9a4218-71e2-4587-d1c5-80c71d397a43"
      },
      "source": [
        "def topk_sampling(seed_text, top_k=100, filter_value=-float('Inf')): \n",
        "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
        "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
        "\n",
        "  # create the tensor to store the model input (initially the input is just the seed text)\n",
        "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for _ in range(num_words_to_generate): # run over number of word to generate\n",
        "      # prepare gpt-2 model input\n",
        "      inputs = {'input_ids': generated}\n",
        "\n",
        "      # feed input to the model\n",
        "      outputs = model(**inputs)[0]\n",
        "\n",
        "      # extract the next token logits (unormalized probability distribution)\n",
        "      next_token_logits = outputs[:, -1, :] \n",
        "      \n",
        "      # Safety check just in case number of words in the vocabulary is less than top_k, \n",
        "      # we need to set top_k to number of words in the vocabulary\n",
        "      top_k = min(top_k, next_token_logits.size(-1))  \n",
        "\n",
        "      # Remove all tokens with a probability less than the last token of the top-k list\n",
        "      indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "      next_token_logits[indices_to_remove] = filter_value # substitute negative infinity so that those words would never be sampled\n",
        "\n",
        "      # Sample from top-k probability distribution\n",
        "      next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "      \n",
        "      # add the generated token to the input\n",
        "      generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "  # convert the model generation from token ids to raw text\n",
        "  generated = generated[:, len(seed_tokens):].tolist() # seed tokens are already in raw form\n",
        "  for g in generated: # for every generated token\n",
        "    text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  return text\n",
        " \n",
        "print(\"top-k sampling's seed text = %s\"%SEED_TEXT)\n",
        "print(\"top-k sampling's continuation text = %s\"%topk_sampling(SEED_TEXT, top_k=40))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top-k sampling's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The\n",
            "top-k sampling's continuation text =  study was published in the journal Philosophical Transactions of the Royal Society B.\n",
            "\n",
            "But despite that astonishing discovery, scientists still think they have a long way to go before researchers believe they have had a unicorn.\n",
            "\n",
            "According to John D. Hines, an ecologist at the University of Georgia Institute of Technology, \"there were no studies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3lDVDUHZxFZ"
      },
      "source": [
        "### Top-p (or nucleus) sampling\n",
        "\n",
        "On the other hand, the nucleus (top-p) sampler restricts sampling to the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1).\n",
        "\n",
        "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5EnQdZC0GEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab2975f-65f6-4ebf-b319-b0960b1553f4"
      },
      "source": [
        "# create the tensor to store the model input (initially the input is just the seed text)\n",
        "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(generated)\n",
        "print(generated.size())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "torch.Size([1, 53])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzCnMoKv0IXG"
      },
      "source": [
        "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX4ZSFoi0Igc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c32cbe-1359-4df7-dbc0-da2eab809fa6"
      },
      "source": [
        "# prepare gpt-2 model input\n",
        "inputs = {'input_ids': generated}\n",
        "\n",
        "# feed input to the model\n",
        "outputs = model(**inputs)[0]\n",
        "\n",
        "print(outputs.size()) # batch size x number of tokens in input x number of tokens in GPT-2 vocabulary"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 53, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW7G6o6k0L0t"
      },
      "source": [
        "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzhnxJsS0N4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef359d21-9357-47df-f622-b6f432b9b43d"
      },
      "source": [
        "# extract the next token logits (unnormalized probability distribution)\n",
        "next_token_logits = outputs[:, -1, :]\n",
        "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7fjM2ZC0SzG"
      },
      "source": [
        "**In top-p sampling, we sample from the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1) as shown below in an example.**\n",
        "\n",
        "<img src=\"\n",
        "https://drive.google.com/uc?id=1P3n2ygqxEQsYre-i_CvWQAOsqlzdho4Z\" alt=\"top-p sampling\" title=\"top-p sampling\"  height=270 />\n",
        "\n",
        "So we first compute the cummulative probability (the last column in the right most table in the figure).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZnyYOvs1nig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576fc525-7256-4e52-a20f-2bfc40018347"
      },
      "source": [
        "# compute the cummulative probability\n",
        "sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True) # sort words based on probability\n",
        "cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "print(cumulative_probs.size()) # 1 x number of tokens in GPT-2 vocabulary"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNGZ5pgV2a1n"
      },
      "source": [
        "And the we remove tokens with cumulative probability above the given threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIl1SpAz2fbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ea9797-f687-498e-c7f9-49f00ffb1a42"
      },
      "source": [
        "# Remove tokens with cumulative probability above the threshold\n",
        "sorted_indices_to_remove = cumulative_probs > 0.9 # top-p=0.9\n",
        "# Shift the indices to the right to keep also the first token above the threshold\n",
        "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "sorted_indices_to_remove[..., 0] = 0\n",
        "# scatter sorted tensors to original indexing\n",
        "indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "next_token_logits[indices_to_remove] = -float('Inf') # substitute negative infinity so that those words would never be sampled\n",
        "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrr2zlWH3YTK"
      },
      "source": [
        "Now we can sample from top-p probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zszufgAq3c0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c2c696-8556-44d2-8a2f-978c54145cf7"
      },
      "source": [
        "# Sample from top-p probability distribution\n",
        "next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "print(next_token) # id for the predicted token     "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2050]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0t_dZW33i8o"
      },
      "source": [
        "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEYjmBj3nM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740d5d5e-86e9-4d8d-f40a-041b97ae6d06"
      },
      "source": [
        "print('before appending the new generated token...')\n",
        "print(generated)\n",
        "# add the generated token to the input\n",
        "generated = torch.cat((generated, next_token), dim=1)\n",
        "print('after appending the new generated token...')\n",
        "print(generated)\n",
        "print('our text (seed text + generated text) so far...')\n",
        "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383]], device='cuda:0')\n",
            "after appending the new generated token...\n",
            "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
            "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
            "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
            "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
            "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
            "          3594,    13,   383,  2050]], device='cuda:0')\n",
            "our text (seed text + generated text) so far...\n",
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R25nS4xb3pLc"
      },
      "source": [
        "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
        "\n",
        "And the full fledged code for top-p sampling will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iguauNBhZxFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafc8523-8701-4ad2-d392-3280d5af2941"
      },
      "source": [
        "def top_p_sampling(seed_text, top_p=0.9, filter_value=-float('Inf')):  \n",
        "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
        "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
        "  \n",
        "  # create the tensor to store the model input (initially the input is just the seed text)\n",
        "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for _ in range(num_words_to_generate):\n",
        "      # prepare gpt-2 model input\n",
        "      inputs = {'input_ids': generated}\n",
        "\n",
        "      # feed input to the model\n",
        "      outputs = model(**inputs)[0]\n",
        "\n",
        "      # extract the next token logits (unormalized probability distribution)\n",
        "      next_token_logits = outputs[:, -1, :]\n",
        "      \n",
        "      # compute the cummulative probability\n",
        "      sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "      cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "      # Remove tokens with cumulative probability above the threshold\n",
        "      sorted_indices_to_remove = cumulative_probs > top_p\n",
        "      # Shift the indices to the right to keep also the first token above the threshold\n",
        "      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "      sorted_indices_to_remove[..., 0] = 0\n",
        "      # scatter sorted tensors to original indexing\n",
        "      indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "      next_token_logits[indices_to_remove] = filter_value # substitute negative infinity so that those words would never be sampled\n",
        "\n",
        "      # Sample from top-p probability distribution\n",
        "      next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "      \n",
        "      # add the generated token to the input\n",
        "      generated = torch.cat((generated, next_token), dim=1)\n",
        "  \n",
        "  # convert the model generation from token ids to raw text\n",
        "  generated = generated[:, len(seed_tokens):].tolist() \n",
        "  for g in generated:\n",
        "    text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
        "  \n",
        "  return text\n",
        "\n",
        "print(\"nucleus (top-p) sampling's seed text = %s\"%SEED_TEXT)\n",
        "print(\"nucleus (top-p) sampling's continuation text = %s\"%top_p_sampling(SEED_TEXT, top_p=0.9))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nucleus (top-p) sampling's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
            "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
            "researchers was the fact that the unicorns spoke perfect English. The\n",
            "nucleus (top-p) sampling's continuation text =  Langans knew one such unicorn -- a seabird like one created by their ancestors from India.\n",
            "\n",
            "Sixty-four years later, archaeologists will unveil the first evidence of a \"real historical living ancestor.\"\n",
            "\n",
            "This fossil found near a city in the Patagonian border region of the Andes, National Geographic reports.\n",
            "\n",
            "The\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXOlRUtYdzBF"
      },
      "source": [
        "That's it!\n",
        "\n",
        "If you're curious, try out the online GPT-2 demo: https://transformer.huggingface.co/doc/gpt2-large"
      ]
    }
  ]
}