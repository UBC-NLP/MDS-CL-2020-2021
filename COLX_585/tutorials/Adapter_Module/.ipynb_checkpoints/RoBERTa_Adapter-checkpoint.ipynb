{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-5K8_75JUS7"
   },
   "source": [
    "# COLX 585 Trends in Computational Linguistic\n",
    "\n",
    "## Lab tutorial: RoBERTa with Adapter Module\n",
    "\n",
    "Traditional fine-tuning can effectively transfer the knowledge of pre-trained language models (e.g., BERT and RoBERTa) to a task-specific task, however, this full fine-tuning strategy is parameter ineffective because it backpropogates through all the layers and updates all the parameters of a model--very time consuming. Hence, the [adapater module](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf) was introduced to improve the fine-tuning efficency without dropping performance levels. \n",
    "\n",
    "As the following figure shows, the adapater module is an additional module added after each pre-trained sublayer in the standard Transformer architecture. Each adapter module is a small feed-forward neural network where hidden size is much smaller than Transformer's hidden size. During downstream task fine-tuning, we freeze the parameters of pre-trained layers and only optimze the parameters of adapter modules. Hence, the number of training parameters is reduced significantly.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-0apxL1ksuA"
   },
   "source": [
    "![](https://miro.medium.com/max/570/0*Z2FMWTCmdkgevHr-.png)\n",
    "\n",
    "Picture Courtesy: Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019, May). [Parameter-efficient transfer learning for NLP](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf). In International Conference on Machine Learning (pp. 2790-2799). PMLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l9u05kOp4n4"
   },
   "source": [
    "In this tutorial, we use an off-the-shelf PyTorch library, [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers), to add the adapter module to RoBERTa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVNJQPYo9JU8",
    "outputId": "e98da6c7-d9f6-4559-c27b-d18e31735b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
      "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "Successfully installed torch-1.4.0\n",
      "ERROR: unknown command \"transformers==3.1.0\"\n",
      "Collecting adapter-transformers==1.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/8a/5a4cd4ed09201f76d5eb6d7a36231bc98da2bfa28e2d03c7abfafcdf6baf/adapter_transformers-1.1.1-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 16.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (3.12.4)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 55.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (1.19.5)\n",
      "Collecting tokenizers==0.9.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 53.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (4.41.1)\n",
      "Collecting sentencepiece==0.1.91\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 50.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->adapter-transformers==1.1.1) (54.2.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->adapter-transformers==1.1.1) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==1.1.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==1.1.1) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==1.1.1) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=675e0be5c5ab4621e36b2f3d80e9e2a36a7316d377f23cf4b33f2bb81cf4372d\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, sentencepiece, adapter-transformers\n",
      "Successfully installed adapter-transformers-1.1.1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3\n"
     ]
    }
   ],
   "source": [
    "# adapter-transformers library is based on torch==1.4.0 and transformers==3.1.0.\n",
    "!pip install torch==1.4.0\n",
    "!pip transformers==3.1.0\n",
    "!pip install adapter-transformers==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dBkvK8qF0uu"
   },
   "source": [
    "## Import required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cJuQa7eoyt2U"
   },
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaConfig, RobertaModelWithHeads, RobertaTokenizer, AdapterType, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKMVNtTvGKmw"
   },
   "source": [
    "## Set seed of randomization and working device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "st54LckLtikU",
    "outputId": "146ae2f6-edf4-44fe-93c4-6028242ac04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_T0HnAjHID-"
   },
   "source": [
    "### Define data generator class and preparation function.\n",
    "\n",
    "The custom dataset should inherit [`Dataset`](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) and define the following methods:\n",
    "  * `__len__` so that len(dataset) returns the size of the dataset.\n",
    "  * `__getitem__` to support the indexing such that `dataset[i]` can be used to get $i$th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LM1Nuy0-zE5m"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # initialization\n",
    "    def __init__(self, dataframe, tokenizer, max_len, lab2ind):\n",
    "        \"\"\"\n",
    "          dataframe: pandas DataFrame.\n",
    "          tokenizer: Hugginfance BERT/RoBERTa tokenizer\n",
    "          max_len: maximal length of input sequence\n",
    "          lab2ind: dictionary of label classes\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = self.data.content\n",
    "        self.labels = self.data.label\n",
    "        self.max_len = max_len\n",
    "        self.lab2ind = lab2ind\n",
    "\n",
    "    # get the size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    # generate sample by index\n",
    "    def __getitem__(self, index):\n",
    "        # get ith sample and label\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        label = str(self.labels[index])\n",
    "\n",
    "        label = self.lab2ind[label]\n",
    "        # use encode_plus() of Transformers to tokenize and vectorize input seuqnce and covert it to tensors. \n",
    "        # this method truncate or pad sequence to the maximal length and then return pytorch tensors. \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': inputs['input_ids'],\n",
    "            'mask': inputs['attention_mask'],\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgAYmEtjNklX"
   },
   "source": [
    "### Define a function to load datasets and create data iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kKUn3JhzNdb_"
   },
   "outputs": [],
   "source": [
    "def regular_encode(file_path, tokenizer, lab2ind, shuffle=True, num_workers = 2, batch_size=64, maxlen = 32, mode = 'train'): \n",
    "    '''\n",
    "      file_path: path to your dataset file\n",
    "      tokenizer: tokenizer method\n",
    "      lab2ind: label-to-index dictionary\n",
    "      shuffle: shuffle the dataset or not\n",
    "      num_workers: a number of data processors\n",
    "      batch_size: the number of batch size\n",
    "      maxlen: maximal sequence length\n",
    "      mode: the type of dataset\n",
    "    '''\n",
    "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
    "    if mode == 'train':\n",
    "        # Use pandas to load dataset, the dataset should be a tsv file where the first line is the header.\n",
    "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content','label'], encoding='utf-8', quotechar=None, quoting=3)\n",
    "    \n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    elif mode == 'predict':\n",
    "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content', 'label'])\n",
    "    else:\n",
    "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
    "        return\n",
    "        \n",
    "    print(\"{} Dataset: {}\".format(file_path, df.shape))\n",
    "    # instantiate the dataset instance \n",
    "    custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind)\n",
    "    \n",
    "    dataset_params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers}\n",
    "\n",
    "    batch_data_loader = DataLoader(custom_set, **dataset_params)\n",
    "    # return a data iterator\n",
    "    return batch_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgBDJUSYORlw"
   },
   "source": [
    "### Training and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eKKJ1AJtOQi2"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for _, batch in enumerate(iterator):\n",
    "        # load data batch\n",
    "        input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        input_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['targets'].to(device, dtype = torch.long)\n",
    "        # forward\n",
    "        outputs = model(input_ids, input_mask, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        # delete used variables to free GPU memory\n",
    "        del batch, input_ids, input_mask, labels\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        if torch.cuda.device_count() == 1:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            epoch_loss += loss.cpu().item()\n",
    "        else:\n",
    "            loss.mean().backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            epoch_loss += loss.mean().cpu().item()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    # free GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator, 0):\n",
    "        # Add batch to GPU\n",
    "            input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            input_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['targets'].to(device, dtype = torch.long)\n",
    "            # forward\n",
    "            outputs = model(input_ids, input_mask, labels=labels)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # delete used variables to free GPU memory\n",
    "            del batch, input_ids, input_mask\n",
    "\n",
    "            if torch.cuda.device_count() == 1:\n",
    "                epoch_loss += loss.cpu().item()\n",
    "            else:\n",
    "                epoch_loss += loss.sum().cpu().item()\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(labels.cpu())\n",
    "    # computing metrics \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    recall = recall_score(all_label, all_pred, average='macro')\n",
    "    precision = precision_score(all_label, all_pred, average='macro')\n",
    "\n",
    "    return epoch_loss/len(iterator), accuracy, f1score, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iFQAgsrOZTD"
   },
   "source": [
    "### Create a optimizer and scheduler.\n",
    "\n",
    "The model train with a linear learing rate [scheduler](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) that decreases linearly from the peak learning rate to 0 after a warmup period where learning rate linearly increase from 0 to the peak learning rate. \n",
    "\n",
    "![](https://huggingface.co/transformers/_images/warmup_linear_schedule.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "povIm1kmOZkZ"
   },
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(model, num_training_steps, warmup_steps, learning_rate):\n",
    "    \"\"\"\n",
    "    Setup the optimizer and the learning rate scheduler.\n",
    "    num_training_steps: the number of training steps\n",
    "    warmup_steps: the number of warm-up steps\n",
    "    learning_rate: the peak learning rate\n",
    "    \"\"\"\n",
    "    optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OnHWgBdJvC6"
   },
   "source": [
    "### Train Adapter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHgthNPNPlck"
   },
   "source": [
    "Load model and tokenizer of `RoBERTa-Base` by the shortcut name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKZk5Ctd1xF8",
    "outputId": "844cf28d-9036-406b-cac8-37dae7697a8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModelWithHeads.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63utOxW56H99",
    "outputId": "875fbc40-9914-42ed-cf4b-6d60ba478144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roberta-base has 124,645,632 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The roberta-base has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6mnmERrP2Il"
   },
   "source": [
    "Add task-specific adapter module for sociality classification task that is a single text classification task. By calling `train_adapter([\"social\"])`, we freeze all transformer parameters and only optimize the parameters of `social` adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tqFygAltPhub"
   },
   "outputs": [],
   "source": [
    "model.add_adapter(\"social\", AdapterType.text_task)\n",
    "model.train_adapter([\"social\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJp-Sz8VQDNl"
   },
   "source": [
    "Add the classification head, i.e., a two-layer feed-forward neural network, on top of the Transformer layers. \n",
    "\n",
    "The method `model.set_active_adapters([[\"social\"]])` registers the `social` adapter as a default for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mqGjybRjee98"
   },
   "outputs": [],
   "source": [
    "model.add_classification_head(\"social\", num_labels=2)\n",
    "model.set_active_adapters([[\"social\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdBm0YSi4MnW"
   },
   "source": [
    "Send model to device (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YM9ND0jdlAN_"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CbM0uPSKXV2",
    "outputId": "c0d31180-45cf-4c7d-80f9-b6d9dfc6ab53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModelWithHeads(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_text_task_adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "              (attention_text_lang_adapters): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "            (layer_text_task_adapters): ModuleDict(\n",
      "              (social): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (layer_text_lang_adapters): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (invertible_lang_adapters): ModuleDict()\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (social): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9R6LSkZbQoTj",
    "outputId": "f267b7ca-f15b-48e7-c14b-2832d3993bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The social adapter RoBERTa model has 1,486,658 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The social adapter RoBERTa model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZyBV1Sg6l2b"
   },
   "source": [
    "The number of trainable parameters decrease significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPiPgEj66xyH"
   },
   "source": [
    "### Training\n",
    "\n",
    "Specify hyper-parameters and load datasets.\n",
    "\n",
    "We freeze all the Transformer layers and only optimize the parameters of adapter modules that are new added and randomly initialized. Hence, we use large learning rate (i.e., 3e-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JQ-zE5YmoNy7"
   },
   "outputs": [],
   "source": [
    "lab2ind = {'no': 0, 'yes': 1}\n",
    "batch_size = 32\n",
    "max_seq_length = 32\n",
    "num_epochs = 5\n",
    "warmup_proportion = 0.1\n",
    "learning_rate = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "data_dir = \"./drive/My Drive/Colab Notebooks/happy_db/\"\n",
    "\n",
    "train_file = os.path.join(data_dir, \"train.tsv\")\n",
    "dev_file = os.path.join(data_dir, \"dev.tsv\")\n",
    "test_file = os.path.join(data_dir, \"test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Dw-Ewa76Pm"
   },
   "source": [
    "Create data interators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Arw0qqhqmKzn",
    "outputId": "e29f4efd-6dfb-402e-aa36-ee3fa2751a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./drive/My Drive/Colab Notebooks/happy_db/train.tsv Dataset: (8448, 2)\n",
      "./drive/My Drive/Colab Notebooks/happy_db/dev.tsv Dataset: (1056, 2)\n",
      "./drive/My Drive/Colab Notebooks/happy_db/test.tsv Dataset: (1056, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = regular_encode(train_file, tokenizer, lab2ind, shuffle=True, batch_size=batch_size, maxlen = max_seq_length)\n",
    "validation_dataloader = regular_encode(dev_file, tokenizer, lab2ind, shuffle=False, batch_size=batch_size, maxlen = max_seq_length)\n",
    "test_dataloader = regular_encode(test_file, tokenizer, lab2ind, shuffle=False, batch_size=batch_size, maxlen = max_seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMpFRRvN7-pJ"
   },
   "source": [
    "Optimizer, sheduler, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "LI1eJRN6pH1l"
   },
   "outputs": [],
   "source": [
    "num_training_steps\t= len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = num_training_steps * warmup_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "skOVX76SqU0I"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler = create_optimizer_and_scheduler(model, num_training_steps, num_warmup_steps, learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QZR9g0T8LBD"
   },
   "source": [
    "Train the model with 10 epochs. The training speed is much faster than fully fine-tuning (i.e., optimize the parameters of the entire RoBERTa). We save the `social` adapter module at the end of each epoach rather than the entire RoBERTa model. This `social` adapter is light weight: it is only 3MB! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmHkqZquq-FO",
    "outputId": "6e452dad-28d9-45a5-e20f-18c5a6e181e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch:  20%|██        | 1/5 [00:32<02:09, 32.35s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 1, 'train_loss': 0.14573501964861696, 'val_acc': 0.9365530303030303, 'val_recall': 0.9382860040567951, 'val_precision': 0.9351728974483466, 'val_f1': 0.9361959428079305}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  40%|████      | 2/5 [01:05<01:37, 32.58s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 2, 'train_loss': 0.13211637516647126, 'val_acc': 0.9412878787878788, 'val_recall': 0.9431614024920313, 'val_precision': 0.93992981144016, 'val_f1': 0.9409657978165136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  60%|██████    | 3/5 [01:39<01:06, 33.02s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 3, 'train_loss': 0.11951439521473015, 'val_acc': 0.9384469696969697, 'val_recall': 0.9401984931903796, 'val_precision': 0.9370718023412634, 'val_f1': 0.9381005415300818}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  80%|████████  | 4/5 [02:14<00:33, 33.69s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 4, 'train_loss': 0.11095547920558602, 'val_acc': 0.9384469696969697, 'val_recall': 0.9401984931903796, 'val_precision': 0.9370718023412634, 'val_f1': 0.9381005415300818}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: 100%|██████████| 5/5 [02:49<00:00, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch_num': 5, 'train_loss': 0.1109762998989247, 'val_acc': 0.9384469696969697, 'val_recall': 0.9401984931903796, 'val_precision': 0.9370718023412634, 'val_f1': 0.9381005415300818}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_res = []\n",
    "\n",
    "for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "    train_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n",
    "    val_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n",
    "\n",
    "    epoch_eval_result = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n",
    "                      \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1\n",
    "                      }\n",
    "    print(epoch_eval_result)\n",
    "    epoch_res.append(epoch_eval_result)\n",
    "    save_path = \"./epoch\"+str(epoch+1) \n",
    "    if os.path.exists(save_path) == False:\n",
    "      os.makedirs(save_path)\n",
    "\n",
    "    model.save_all_adapters(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQqG5JOd8oFF"
   },
   "source": [
    "Present the validation preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "8cah3xlmrWen",
    "outputId": "7c6c1740-f497-4dbe-cb21-2cc703561228"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch_num</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.941288</td>\n",
       "      <td>0.943161</td>\n",
       "      <td>0.939930</td>\n",
       "      <td>0.940966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.119514</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>0.937072</td>\n",
       "      <td>0.938101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.110955</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>0.937072</td>\n",
       "      <td>0.938101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.110976</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>0.937072</td>\n",
       "      <td>0.938101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.145735</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.938286</td>\n",
       "      <td>0.935173</td>\n",
       "      <td>0.936196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch_num  train_loss   val_acc  val_recall  val_precision    val_f1\n",
       "1          2    0.132116  0.941288    0.943161       0.939930  0.940966\n",
       "2          3    0.119514  0.938447    0.940198       0.937072  0.938101\n",
       "3          4    0.110955  0.938447    0.940198       0.937072  0.938101\n",
       "4          5    0.110976  0.938447    0.940198       0.937072  0.938101\n",
       "0          1    0.145735  0.936553    0.938286       0.935173  0.936196"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = pd.DataFrame(epoch_res)\n",
    "report_df.sort_values(by=[\"val_f1\"], ascending=False, inplace=True)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVSTo5cpJ1-Q"
   },
   "source": [
    "### Load the best adapter model and evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJNvbRHi8_AH"
   },
   "source": [
    "Load the pre-trained RoBERTa model by shortcut name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rh3Sath22S4L",
    "outputId": "0bf6ce7d-95f5-4151-f6fe-688b22ddf504"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModelWithHeads.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ysokZo4Q9UC"
   },
   "source": [
    "Load the trained task-specific adapter module that achieves the best performance on validation set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBe8hu6kDiLr",
    "outputId": "40a90745-d0fe-427e-cf00-3e16714a6b1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'social'.\n",
      "Overwriting existing head 'social'\n"
     ]
    }
   ],
   "source": [
    "adapter_name = model.load_adapter(\"./epoch2/social\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjalnhFTCah-"
   },
   "source": [
    "Add the trained adapter to RoBERTa and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qtNwttliDqkA"
   },
   "outputs": [],
   "source": [
    "model.set_active_adapters(adapter_name)\n",
    "model = model.to(device)\n",
    "test_loss, test_acc, test_f1, test_recall, test_precision = evaluate(model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAWfJ-AoD1Ch",
    "outputId": "e28cefc1-108c-4216-fd1a-7b74cdc00d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2498893676833673 0.9204545454545454 0.9199480181936321 0.9219626778024272 0.918901412100013\n"
     ]
    }
   ],
   "source": [
    "print(test_loss, test_acc, test_f1, test_recall, test_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiF6PiRSDItt"
   },
   "source": [
    "### References:\n",
    "* Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019, May). [Parameter-efficient transfer learning for NLP](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf). In International Conference on Machine Learning (pp. 2790-2799). PMLR.\n",
    "\n",
    "* https://docs.adapterhub.ml/index.html\n",
    "\n",
    "* https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpXZOuUUDTmq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RoBERTa_Adapter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
