{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "domain_adversarial_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es8eow5fwowj"
      },
      "source": [
        "## <font color=green> Domain Adaptation using Domain Adversarial Neural Networks </font>\n",
        "\n",
        "### <font color=blue> Goal of this tutorial: </font>\n",
        "- Know the background of Domain Adaptation and Domain Adversarial Neural Networks (DANN)\n",
        "- Implement DANN for sentiment domain adaptation using PyTorch\n",
        "\n",
        "###  <font color=blue> General: </font>\n",
        "- This notebook was last tested on Python 3.6.4, PyTorch 0.4.0, scikit-learn 0.21.3\n",
        "- We would like to acknowledge the DANN repository from fungtion (https://github.com/fungtion/DANN) which we used as a reference to code up the DANN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhqqzyztwowm"
      },
      "source": [
        "## <font color=green> Background </font>\n",
        "\n",
        "### <font color=blue> Problem: </font>\n",
        "Domain adapation is the problem of learning a machine learning classifier (say) in a domain with no labeled examples. The straightforward approach to build a classifier in this setting is to manually annotate some instances in the domain. Unfortunately, annotation work is either time consuming or expensive or both in most cases. \n",
        "\n",
        "### <font color=blue> How about leveraging labeled examples from a related domain?  </font>\n",
        "That's a good start. We can maybe train our classifier on the labeled domain (let's call it source domain) and evaluate on the unlabled domain of interest (let's call it target domain). This approach mostly likely does not work as there is a shift in the distribution of data in the training set from the test set. In machine learning terms, we say that the approach breaks the i.i.d (independently sampled and identically distributed) assumption of machine learning models that the distribution of the training data and the test data should be identical.\n",
        "\n",
        "### <font color=blue> What would be a good application of domain adaptation? </font>\n",
        "Let's say we want to build a binary (+ve/-ve) sentiment classifier in Automobile domain (target domain). Although we generally have a lot of unlabeled reviews in Automobile domain, we don't have any labeled examples from this domain to work with. Maybe we do have access to labeled sentiment reviews (+ve/-ve) from a related domain, Books (source domain). So the problem becomes: given labeled examples from a source domain (Books) and unlabeled examples from a target domain (Automobile), how do we go about building a sentiment classifier for the target domain (Automobile) which is reasonable? \n",
        "\n",
        "### <font color=blue> What's a reasonable model for domain adaptation? </font>\n",
        "A model which achieves good accuracy is generally reasonable. So we test the predictions of our domain adaption model on few labeled examples on the target domain (Automobile) to measure the performance. Note that the labeled examples in the target domain should be treated as the test set and hence we should always follow the golden rule of machine learning (i.e., never use them to influence our model in anyway).\n",
        "\n",
        "### <font color=blue> Domain Adversarial Neural Networks: </font>\n",
        "<img src=\"images/dann_architecture.jpeg\" alt=\"DANN Architecture\" title=\"DANN - Architecture\" />\n",
        "Domain Adversarial Neural Networks tries to solve this problem by training a model that simulatenously does two things: i) learns representation that makes examples from source and target domain appear similarly (domain classification) and ii) simultaneously optimizes the representation for achieving minimal error in classifying labeled samples from the source domain (source classification). We can imagine a 1 hidden layer neural network which \n",
        "1. takes the example as input (e.g., tf-idf vector for a review)\n",
        "2. passes it to affine transformation followed by non-linearity (e.g, sigmoid) to compute hidden representation\n",
        "3. passes the hidden representation to a classifier that predicts the sentiment label of the source example WELL. (minimizing the source classification error)\n",
        "4. passes the same hidden representation to another classifier that predicts the domain of the input example BADLY. (maximizing the domain classification error)\n",
        "- The objective in iii) ensures the internal representation of neural network contains no discriminative information about the domain of the input. The other objective in iv) preserves low error rate on classification of source samples.  And the intuition of DANN is to make the source accuracy correspond to target accuracy when the source and target domain distributions are made similar.\n",
        "\n",
        "### <font color=blue> References: </font>\n",
        "To know more about DANN, take a look at the following articles:\n",
        "1. Domain-Adversarial Neural Networks https://arxiv.org/pdf/1412.4446.pdf\n",
        "2. Unsupervised Domain Adaptation by Backpropagation http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf\n",
        "\n",
        "\n",
        "Let's do all the imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_uupzwQwown"
      },
      "source": [
        "'''\n",
        "One place for all the imports\n",
        "'''\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from collections import Counter\n",
        "import random\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Function\n",
        "\n",
        "# set the seed\n",
        "manual_seed = 123\n",
        "random.seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "  torch.cuda.manual_seed(manual_seed)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmba4A5NxteY"
      },
      "source": [
        "Prepare the Amazon Movie Reviews Dataset\n",
        "- We'll try to stick to the specs. mentioned in https://arxiv.org/pdf/1412.4446.pdf\n",
        "- We'll focus on adapation from Books to DVD domain.\n",
        "- We'll use the preprocessed dataset from http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\n",
        "- Download link: http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBkmsREix4EV",
        "outputId": "1df2b3c8-9e58-45ff-e949-57a3890412e9"
      },
      "source": [
        "!wget http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\n",
        "!tar -xvf processed_acl.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-03 03:30:24--  http://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\n",
            "Resolving www.cs.jhu.edu (www.cs.jhu.edu)... 128.220.13.64\n",
            "Connecting to www.cs.jhu.edu (www.cs.jhu.edu)|128.220.13.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19633323 (19M) [application/x-gzip]\n",
            "Saving to: ‘processed_acl.tar.gz.1’\n",
            "\n",
            "processed_acl.tar.g 100%[===================>]  18.72M  8.08MB/s    in 2.3s    \n",
            "\n",
            "2021-04-03 03:30:28 (8.08 MB/s) - ‘processed_acl.tar.gz.1’ saved [19633323/19633323]\n",
            "\n",
            "processed_acl/\n",
            "processed_acl/dvd/\n",
            "processed_acl/dvd/negative.review\n",
            "processed_acl/dvd/unlabeled.review\n",
            "processed_acl/dvd/positive.review\n",
            "processed_acl/books/\n",
            "processed_acl/books/negative.review\n",
            "processed_acl/books/unlabeled.review\n",
            "processed_acl/books/positive.review\n",
            "processed_acl/kitchen/\n",
            "processed_acl/kitchen/negative.review\n",
            "processed_acl/kitchen/unlabeled.review\n",
            "processed_acl/kitchen/positive.review\n",
            "processed_acl/electronics/\n",
            "processed_acl/electronics/negative.review\n",
            "processed_acl/electronics/unlabeled.review\n",
            "processed_acl/electronics/positive.review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjGCsll2wowo",
        "outputId": "612f43eb-f2da-4be5-9255-e76aba917948"
      },
      "source": [
        "# location of the data and size of the vocabulary\n",
        "DATA_DIR = \"processed_acl\"\n",
        "VOCAB_SIZE = 5000\n",
        "\n",
        "# creates the vocab from the preprocessed features\n",
        "def create_vocab(documents):\n",
        "  # count all the tokens in both the files (document frequency)\n",
        "  vocab_count = Counter()\n",
        "  for doc in documents:\n",
        "    doc = doc.strip()\n",
        "    tokens = [token.split(\":\")[0] for token in doc.split()[0:-1]] # last token is the label so we ignore it\n",
        "    for token in set(tokens):\n",
        "      vocab_count[token] += 1\n",
        "  # create the token to id and id to token mappings\n",
        "  t2i, i2t = {}, {}\n",
        "  for token, _ in vocab_count.most_common()[:VOCAB_SIZE]:\n",
        "    t2i[token] = len(i2t)\n",
        "    i2t[t2i[token]] = token\n",
        "  print(\"created vocab. of size %d\"%len(t2i))\n",
        "  print(\"top 10 tokens ...\")\n",
        "  print(list(t2i)[0:10])\n",
        "  return t2i, i2t, vocab_count\n",
        "\n",
        "# create the sparse tf-idf representation from the reviews\n",
        "def tfidf_docs(documents, t2i, i2t, vocab_count):\n",
        "  coords_1, coords_2, values, y = [], [], [], []\n",
        "  row_id = 0\n",
        "  n, d = len(documents), len(t2i)\n",
        "  for doc in documents:\n",
        "    items = doc.split()\n",
        "    for item in items[0:-1]:\n",
        "      token, freq = item.split(\":\")\n",
        "      if token in t2i:\n",
        "        col_id = t2i[token]\n",
        "        # we will use the weighing scheme 2 from the recommended options \n",
        "        # ref: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "        tf_score = 1.0 + math.log(float(freq))\n",
        "        idf_score = math.log(1.0 + float(n)/vocab_count[token])\n",
        "        coords_1.append(row_id)\n",
        "        coords_2.append(col_id)\n",
        "        values.append(tf_score*idf_score)\n",
        "    label = 1 if items[-1].split(\":\")[1] == \"positive\" else 0\n",
        "    y.append(label)\n",
        "    row_id = row_id + 1\n",
        "  X = sparse.coo_matrix((values, (coords_1, coords_2)), shape=(n, d))\n",
        "  y = np.array(y)\n",
        "  print(\"shape of inputs = %d, %d\"%(X.shape[0], X.shape[1]))\n",
        "  print(\"number of non-zero entries = %d\"%(X.count_nonzero()))\n",
        "  return X, y\n",
        "    \n",
        "# reads the labeled documents to X, y\n",
        "def read_labeled_dir(domain, vocab=None):\n",
        "  print(\"processing the labeled %s domain\"%domain)\n",
        "  \n",
        "  # file paths\n",
        "  positive_f = os.path.join(DATA_DIR, domain, 'positive.review')\n",
        "  negative_f = os.path.join(DATA_DIR, domain, 'negative.review')\n",
        "    \n",
        "  # load both files to memory\n",
        "  positive_documents = [line.strip() for line in open(positive_f)]\n",
        "  negative_documents = [line.strip() for line in open(negative_f)]\n",
        "  total_documents = positive_documents + negative_documents\n",
        "  random.shuffle(total_documents)\n",
        "  \n",
        "  if not vocab:\n",
        "    # read the vocab\n",
        "    t2i, i2t, vocab_count = create_vocab(total_documents)\n",
        "  else:\n",
        "    t2i, i2t, vocab_count = vocab['t2i'], vocab['i2t'], vocab['vocab_count']\n",
        "\n",
        "  # create the tf-idf representation for all the documents\n",
        "  X, y = tfidf_docs(total_documents, t2i, i2t, vocab_count)\n",
        "  \n",
        "  return {'inputs': X, 'labels': y}, {'t2i': t2i, 'i2t': i2t, 'vocab_count': vocab_count}\n",
        "\n",
        "# read the unlabeled documents to X\n",
        "def read_unlabeled_file(domain, vocab):\n",
        "  print(\"processing the unlabeled %s domain\"%domain)\n",
        "  \n",
        "  # file paths\n",
        "  unlab_f = os.path.join(DATA_DIR, domain, 'unlabeled.review')\n",
        "  \n",
        "  # load the content to memory\n",
        "  unlab_documents = [line.strip() for line in open(unlab_f)]\n",
        "  random.shuffle(unlab_documents)\n",
        "  unlab_documents = unlab_documents[0:2000]\n",
        "  \n",
        "  t2i, i2t, vocab_count = vocab['t2i'], vocab['i2t'], vocab['vocab_count']\n",
        "  \n",
        "  # create the tf-idf representation for all the unlabeled documents\n",
        "  X, _ = tfidf_docs(unlab_documents, t2i, i2t, vocab_count)\n",
        "  \n",
        "  return {'inputs': X}\n",
        "\n",
        "# read the labeled data from source domain\n",
        "books_labeled_data, books_vocab = read_labeled_dir(\"books\")\n",
        "\n",
        "# read the labeled data from target domain\n",
        "dvd_labeled_data, _ = read_labeled_dir(\"dvd\", vocab=books_vocab)\n",
        "\n",
        "# read the unlabeled data from target domain\n",
        "dvd_unlabeled_data = read_unlabeled_file(\"dvd\", books_vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing the labeled books domain\n",
            "created vocab. of size 5000\n",
            "top 10 tokens ...\n",
            "['book', 'i', 'this_book', 'not', 'was', 'you', 'read', 'one', 'all', 'about']\n",
            "shape of inputs = 2000, 5000\n",
            "number of non-zero entries = 198264\n",
            "processing the labeled dvd domain\n",
            "shape of inputs = 2000, 5000\n",
            "number of non-zero entries = 175503\n",
            "processing the unlabeled dvd domain\n",
            "shape of inputs = 2000, 5000\n",
            "number of non-zero entries = 174850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWXVftNJykDA"
      },
      "source": [
        "#### Create the PyTorch modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EJu4NHGwowt"
      },
      "source": [
        "# gradient reversal layer\n",
        "class ReverseLayerF(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, lmbda):\n",
        "    ctx.lmbda = lmbda\n",
        "    return x.view_as(x)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    output = grad_output.neg() * ctx.lmbda\n",
        "    return output, None\n",
        "\n",
        "# DANN layers\n",
        "class DANN(nn.Module):\n",
        "  def __init__(self, d, h, lmbda):\n",
        "    super(DANN, self).__init__()\n",
        "    self.lmbda = lmbda\n",
        "    self.input_to_hidden = nn.Linear(d, h)\n",
        "    self.class_classifier = nn.Linear(h, 2) # classes: positive vs negative\n",
        "    self.domain_classifier = nn.Linear(h, 2) # classes: source vs target\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    hidden_rep = self.input_to_hidden(input_data)\n",
        "    class_output = self.class_classifier(hidden_rep)\n",
        "    reverse_feature = ReverseLayerF.apply(hidden_rep, self.lmbda)\n",
        "    domain_output = self.domain_classifier(reverse_feature)\n",
        "    return class_output, domain_output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1RGaXdbzbXI"
      },
      "source": [
        "#### Create the model instance, optimizer, loss and so on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qfPOssiwoww"
      },
      "source": [
        "# hyperparameters for training the model\n",
        "ALPHA = 0.001 # learning rate\n",
        "HIDDEN_SIZE = 200 # search space [1, 5, 12, 25, 50, 75, 100, 150, 200]\n",
        "BATCH_SIZE = 50\n",
        "EPOCHS = 5\n",
        "LAMBDA = 0.1 # search space among 9 values between 10^{−2} and 1 on a logarithmic scale\n",
        "\n",
        "# create the model instance\n",
        "n, d = books_labeled_data[\"inputs\"].shape\n",
        "model = DANN(d, HIDDEN_SIZE, LAMBDA)\n",
        "model.to(device)\n",
        "\n",
        "# setup optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=ALPHA)\n",
        "\n",
        "# setup both the loss\n",
        "loss_class = torch.nn.NLLLoss()\n",
        "loss_domain = torch.nn.NLLLoss()\n",
        "\n",
        "# ensure all the parameters of the model are learnable\n",
        "for p in model.parameters():\n",
        "  p.requires_grad = True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA4xEYugzp5M"
      },
      "source": [
        "#### Train the model on Books reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ATjmnywoww",
        "outputId": "fbc37ea7-5447-428f-8f55-b67ff2ca06cb"
      },
      "source": [
        "# collect the training data\n",
        "X_src = torch.from_numpy(books_labeled_data[\"inputs\"].toarray()).float()\n",
        "y_src_class = torch.from_numpy(books_labeled_data[\"labels\"])\n",
        "X_targ = torch.from_numpy(dvd_unlabeled_data[\"inputs\"].toarray()).float()\n",
        "\n",
        "# placeholders for holding the current batch\n",
        "cur_X_src = torch.cuda.FloatTensor(BATCH_SIZE, d, device=device)\n",
        "cur_y_src_class = torch.zeros(BATCH_SIZE, dtype=torch.long, device=device)\n",
        "cur_y_src_domain = torch.zeros(BATCH_SIZE, dtype=torch.long, device=device)\n",
        "cur_X_targ = torch.cuda.FloatTensor(BATCH_SIZE, d, device=device)\n",
        "cur_y_targ_domain = torch.ones(BATCH_SIZE, dtype=torch.long, device=device)\n",
        "\n",
        "# start training\n",
        "print('training...')\n",
        "model.train()\n",
        "num_batches = n // BATCH_SIZE\n",
        "for epoch in trange(EPOCHS):\n",
        "  rand_idx = np.random.permutation(n)\n",
        "  for bi in range(num_batches):\n",
        "    # prepare batch\n",
        "    for sample_i in range(BATCH_SIZE):\n",
        "      cur_idx = rand_idx[BATCH_SIZE*bi + sample_i]\n",
        "      cur_X_src[sample_i] = X_src[cur_idx]\n",
        "      cur_y_src_class[sample_i] = y_src_class[cur_idx]\n",
        "      cur_X_targ[sample_i] = X_targ[cur_idx]\n",
        "    # train the model using this batch\n",
        "    model.zero_grad() # clears the gradient buffer\n",
        "    # source side losses\n",
        "    class_output, domain_output = model(cur_X_src)\n",
        "    error_src_class = loss_class(class_output, cur_y_src_class)\n",
        "    error_src_domain = loss_domain(domain_output, cur_y_src_domain)\n",
        "    # target side losses\n",
        "    _, domain_output = model(cur_X_targ)\n",
        "    error_src_domain = loss_domain(domain_output, cur_y_targ_domain)\n",
        "    # total losses\n",
        "    error_this_batch = error_src_class + error_src_domain + error_src_domain\n",
        "    # backward prop.\n",
        "    error_this_batch.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  5.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6VOvkez1pN"
      },
      "source": [
        "#### Evaluate the model on DVD reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nlsq1ERwowx",
        "outputId": "7239c838-ed1b-4437-dae4-a0c467d22964"
      },
      "source": [
        "# collect the evaluation data\n",
        "X_targ = torch.from_numpy(dvd_labeled_data[\"inputs\"].toarray()).float()\n",
        "y_targ_class = torch.from_numpy(dvd_labeled_data[\"labels\"])\n",
        "\n",
        "# placeholders for holding the current batch\n",
        "cur_X_targ = torch.FloatTensor(BATCH_SIZE, d, device=device)\n",
        "cur_y_targ_class = torch.zeros(BATCH_SIZE, dtype=torch.long, device=device)\n",
        "\n",
        "model.eval()\n",
        "num_test_instances = X_targ.shape[0]\n",
        "num_batches = num_test_instances // BATCH_SIZE\n",
        "errors = 0.0\n",
        "with torch.no_grad():\n",
        "  for bi in range(num_batches):\n",
        "    # prepare batch\n",
        "    for sample_i in range(BATCH_SIZE):\n",
        "      cur_idx = BATCH_SIZE*bi + sample_i\n",
        "      cur_X_targ[sample_i] = X_targ[cur_idx]\n",
        "      cur_y_targ_class[sample_i] = y_targ_class[cur_idx]\n",
        "    pred_class_output, _ = model(cur_X_targ)\n",
        "    # update errors\n",
        "    for sample_i in range(BATCH_SIZE):\n",
        "      cur_label = 0 if pred_class_output[sample_i][0] > pred_class_output[sample_i][1] else 1\n",
        "      if cur_y_targ_class[sample_i] != cur_label:\n",
        "        errors += 1.0\n",
        "print(\"evaluation error in target labeled samples = %.3f\"%(errors/num_test_instances))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation error in target labeled samples = 0.399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojP0soHY05Sz"
      },
      "source": [
        "That's it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gTtnmz605_s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}