{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cnn_text.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BCGOkdDHfLPl"},"source":["# Convolutional Neural Network for text classification"]},{"cell_type":"markdown","metadata":{"id":"qfkMti7BfLPl"},"source":["Convolutional Neural Network (CNNs) was originally for Computer Vision tasks. [Kim](https://arxiv.org/pdf/1408.5882.pdf) applied CNNs to problems in Natural Language Processing and got good results. In this tutorial, we will introduce how to implement CNN for NLP classification task. "]},{"cell_type":"code","metadata":{"id":"_H05G_RJfvzg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616798342776,"user_tz":420,"elapsed":23650,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"a1f5211c-cf1e-4a84-8f06-f9ac18e1f395"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TUJcy-wij2QV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616798551831,"user_tz":420,"elapsed":164569,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"b3263ab8-0cf8-4839-b6d2-524270bdaf87"},"source":["!pip install -U torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n","\u001b[K     |████████████████████████████████| 804.1MB 22kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement torch==1.8.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.8.0+cu101\n","    Uninstalling torch-1.8.0+cu101:\n","      Successfully uninstalled torch-1.8.0+cu101\n","Successfully installed torch-1.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dJ81nsSohHFQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616798879491,"user_tz":420,"elapsed":6248,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"c37b35f1-ea5c-4dba-f8f9-c79f61267077"},"source":["!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"owf4tLdIfLPm"},"source":["## Import require Python libraries"]},{"cell_type":"code","metadata":{"id":"nt3sgC0BfLPn"},"source":["import torch\n","import torchtext\n","from torchtext.legacy.data import Field, LabelField. # For torch<=0.8.0, the importing of functions should be `from torchtext.data`\n","from torchtext.legacy.data import TabularDataset\n","from torchtext.legacy.data import Iterator, BucketIterator\n","import spacy\n","import en_core_web_sm\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","from tqdm import tqdm, trange\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0UdczZpfLPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799032750,"user_tz":420,"elapsed":494,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"b100181c-e8fc-4bcc-e981-9d6f2687ae2e"},"source":["## Set seed of randomization and working device\n","manual_seed = 77\n","torch.manual_seed(manual_seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","n_gpu = torch.cuda.device_count()\n","if n_gpu > 0:\n","    torch.cuda.manual_seed(manual_seed)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O5CZmExZfLPv"},"source":["## Load dataset"]},{"cell_type":"markdown","metadata":{"id":"FbVtgUT6fLPv"},"source":["In this tutorial, we use the corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazon’s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n","In this tutorial, we focus on `sociality classification`. Sociality refers to `whether or not other people than the author are involved in the emotion situation`. For example, an emotion experience with a sociality value \"yes\" (i.e., other people are involved) could teach us about social groups (e.g., families) and the range of emotions expressed during specific types of situations (e.g., wedding, death). \n","\n","We only use labelled dataset which include 10,560 labelled samples. \n","\n","We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``./happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``. We split the labeled data into 80\\% training set (8,448 moments) and 20\\% development set (2112 moments)."]},{"cell_type":"markdown","metadata":{"id":"7kS9D4uWfLPv"},"source":["Tokenizer"]},{"cell_type":"code","metadata":{"id":"7baIFijufLPw"},"source":["spacy_en = en_core_web_sm.load()\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings (tokens)\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dvUuAsifLPy"},"source":["`Fileds`"]},{"cell_type":"code","metadata":{"id":"coBVxDXOfLPz"},"source":["TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n","LABEL = Field(sequential=False, unk_token = None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVPhbsm3fLP1"},"source":["Load dataset using `TabularDataset`"]},{"cell_type":"code","metadata":{"id":"5rBq5MozfLP2"},"source":["# your code goes here \n","train, val, test = TabularDataset.splits(\n","               path=\"./drive/My Drive/Colab Notebooks/happy_db/\", # the root directory where the data lies\n","               train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n","               format='tsv',\n","               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n","               fields=[('tweet', TEXT), ('label', LABEL)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cC0lP-FfLP5"},"source":["Build your vocabulary to map words and labels to integers. "]},{"cell_type":"code","metadata":{"id":"eyEa098afLP5"},"source":["TEXT.build_vocab(train, min_freq=2)\n","LABEL.build_vocab(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jnYPrC2fLP8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799047536,"user_tz":420,"elapsed":214,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"088ba15d-71c4-4076-bade-9a122dcc30bd"},"source":["print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n","print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocabulary size of TEXT: 3546\n","Vocabulary size of LABEL: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZGmOewZCfLP-"},"source":["Construct the Iterators to get the train, dev, and test splits. Use `BucketIterator` to initialize the Iterators for the train, dev, and test data."]},{"cell_type":"code","metadata":{"id":"-7ANLHxefLP_"},"source":["train_iter, val_iter, test_iter = BucketIterator.splits(\n"," (train, val, test), # we pass in the datasets we want the iterator to draw data from\n"," batch_sizes=(64,256,256),\n"," sort_key=lambda x: len(x.tweet), \n"," sort=True,\n","# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n"," sort_within_batch=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaYfMorVfLQB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799049611,"user_tz":420,"elapsed":248,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"7b92c1c3-44e1-4408-f0ad-d7e36f967771"},"source":["# create a single batch and terminate the loop\n","for batch in train_iter:\n","    tweets = batch.tweet\n","    labels = batch.label\n","    break  #we use first batch as an example.\n","    \n","print('tweets:', tweets.shape)\n","print('labels:', labels.shape)\n","\n","# tweets: [length, batch size]\n","# labels: [batch size]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tweets: torch.Size([4, 64])\n","labels: torch.Size([64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-WMasQ0YfLQG"},"source":["## CNN model for NLP"]},{"cell_type":"markdown","metadata":{"id":"JfCiVeVufLQH"},"source":["Instead of image pixels, the input to most NLP tasks is sentences or documents represented as a matrix. Each row of the matrix corresponds to one token, typically a word, but it could be a character or byte-pair. That is, each row is a vector that represents a word. Usually, these vectors are **word embeddings** such as word2vec or GloVe. You can also train your word embedding layer from scratch. For a 10-word sentence using a 300-dimensional embedding, we would have a `10 × 300` matrix as our input. "]},{"cell_type":"markdown","metadata":{"id":"9dPcLouofLQI"},"source":["In the vision, our kernels slide over local patches of an image, but in NLP, we typically use kernels that slide over full rows of the matrix (words). Thus, the \"width\" of our kernels is the same as the width of the input matrix (i.e, word embedding size). The height (or region size) of the kernels may vary, but sliding windows over 2-5 words at a time is typical. "]},{"cell_type":"markdown","metadata":{"id":"iYje-BUwfLQI"},"source":["**Here is a simple example of a CNN for NLP.**"]},{"cell_type":"markdown","metadata":{"id":"zTBKzYpUfLQJ"},"source":["1. Each input token (words) is represented in a 5-dimension vector. The sentence length is 7. Hence, the input matrix of CNN is a `7 x 5` tensor.\n","2. Here we use three kernel (or filter) region sizes: `2, 3 and 4`, each of which has `2` kernels. Every kernels performs convolution on the sentence matrix and generates (variable-length) feature maps. \n","2. Then `1-max pooling` is performed over each map (i.e., the largest number from each feature map is recorded). Thus a univariate feature vector is generated from all six maps.\n","3. These 6 features are **concatenated** to form a feature vector for the linear layer. \n","4. The final **softmax fully connected layers**, then, receives this feature vector as input and uses it to classify the sentence; here it is a binary classification and hence depict two possible output states. \n","\n","Source: Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification."]},{"cell_type":"markdown","metadata":{"id":"3F3lf-7TfLQK"},"source":["![](http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM-1024x937.png)\n","\n","Picture Courtesy: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"]},{"cell_type":"markdown","metadata":{"id":"Y1RqU28VfLQK"},"source":["**Let's build this model step by step.**"]},{"cell_type":"markdown","metadata":{"id":"15hsz2mgfLQL"},"source":["#### First, we need to create a input tensor and the label tensor. "]},{"cell_type":"code","metadata":{"id":"hzk8z-jxfLQL"},"source":["input_sentence = 'I like this movie very much!'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jB0FeDV_fLQN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799054939,"user_tz":420,"elapsed":292,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"d3b8ce81-b45c-4344-fa8a-dee09cf96fe9"},"source":["text_token = TEXT.preprocess(input_sentence)\n","print(\"text_token:\", text_token)\n","input_tensor = TEXT.process([text_token])\n","print(input_tensor)\n","print(\"shape of input_tensor: \", input_tensor.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["text_token: ['i', 'like', 'this', 'movie', 'very', 'much', '!']\n","tensor([[  2],\n","        [147],\n","        [ 45],\n","        [124],\n","        [ 33],\n","        [189],\n","        [ 63]])\n","shape of input_tensor:  torch.Size([7, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PeUn7e_nfLQR"},"source":["**Create a label tensor.**"]},{"cell_type":"code","metadata":{"id":"qIp_mOccfLQR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799055922,"user_tz":420,"elapsed":226,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"3d52fcfc-e252-42da-dc74-f1c567eba158"},"source":["label_token = LABEL.preprocess('no')\n","print(\"label_token:\", label_token)\n","label_tensor = LABEL.process([label_token])\n","print(\"shape of label_tensor: \", label_tensor.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["label_token: no\n","shape of label_tensor:  torch.Size([1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7zZ9BKEJfLQT"},"source":["#### First layer is a embedding layer which embeds each token to a 5-D vector. \n","\n"," We will use [``torch.nn.Embedding module``](https://pytorch.org/docs/stable/nn.html#embedding) to store word vectors corresponding to words in the vocabulary."]},{"cell_type":"code","metadata":{"id":"A0fNJz4PfLQU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799057020,"user_tz":420,"elapsed":237,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"52a5fc5c-50cd-4a0f-e686-7697c80ea2ac"},"source":["VOCAB_SIZE = len(TEXT.vocab.stoi)\n","print(\"VOCAB_SIZE:\",VOCAB_SIZE)\n","# Note, the parameters to Embedding class below are:\n","# num_embeddings (int): size of the dictionary of embeddings\n","# embedding_dim (int): the size of each embedding vector\n","# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n","embedding = nn.Embedding(VOCAB_SIZE, 5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["VOCAB_SIZE: 3552\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mOsXrRJjfLQX"},"source":["Let us now feed the tensors of our sample batch to the embedding module and extract the sequence of word embeddings for each tweet."]},{"cell_type":"code","metadata":{"id":"kiEyk8MYfLQY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799058188,"user_tz":420,"elapsed":242,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"fd4d5196-4545-4ac5-9646-f986895a0802"},"source":["# print tensor containing word ids for our batch\n","print(\"input_tensor: \", input_tensor.shape)\n","\n","# feed the \"input_tensor\" tensor to the embedding module\n","input_embeddings = embedding(input_tensor)\n","\n","# print the dimensions of the tweet_embeddings\n","print(\"tweet input word embeddings size: \", input_embeddings.size()) \n","\n","# first dimension - batch size (1)\n","# second dimension - number of words in that single example (7)\n","# third dimension - number of features for a word (5) (or word embedding size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input_tensor:  torch.Size([7, 1])\n","tweet input word embeddings size:  torch.Size([7, 1, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"20g9P-tufLQb"},"source":["#### Convolutional layers"]},{"cell_type":"markdown","metadata":{"id":"8Jm399zofLQb"},"source":["This `input_embeddings` is the input tensor of CNN.\n","\n","Our imput is a tensor which include 1 channel and 2 dimensions. So we will use [`nn.Conv2d()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) to convolute the input data. \n","\n","Notice, the input tensor of `nn.Conv2d()` should be size of `[batch size, # of input channel, height, width]`. Here, `height` is the sequence length of our sentence (i.e., 7), and the width is the embedding size (i.e., 5). But the shape of `input_embeddings` is `[sequence length, batch size, embedding size]`. So we need to `permute` the dimensions and `unsqueeze` channel dimension."]},{"cell_type":"code","metadata":{"id":"Ba4SaNBafLQc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799059372,"user_tz":420,"elapsed":162,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"38606f58-f05a-4a84-aebe-a3209002cf06"},"source":["input_embeddings = input_embeddings.permute(1,0,2)\n","print(input_embeddings.shape)\n","input_embeddings = input_embeddings.unsqueeze(1)\n","print(input_embeddings.shape)\n","\n","# fisrt dimension - number of examples (1)\n","# second dimension - number of input channel (1)\n","# first dimension - sequence length (7)\n","# third dimension - word embedding size) (5) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 7, 5])\n","torch.Size([1, 1, 7, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0x_OWc7_fLQd"},"source":["Similarly, \n","\n","To initialize `nn.Conv2d()`, you need to specify some hyper-parameters. \n","\n","The `in_channels` is the number of channel of input tensor. The `out_channels` is the number of channel of output tensor. You can consider `out_channels` as the number of kernels (or filters) we use. In this case, our `in_channels` equal to 1, and we want our outputs in 32 channels.   \n","\n","The `kernel_size` argument is the size of the convolutional kernel. We want kernels with different sized shapes in the height and width directions, we will use a tuple `(height-size, width-size)` (e.g., (2,5)). \n","\n","The `stride` controls the stride for the cross-correlation. We use stride of 1 here.\n","\n","We will not pad the input matrix. \n","\n","But we have three different kernel region sizes: 2, 3 and 4, each of which has 2 kernels. Every kernels performs convolution on the sentence matrix, seperatly. Namely, we need three convolutional layers."]},{"cell_type":"markdown","metadata":{"id":"ZO2UR5YofLQe"},"source":["The first convolutional layer is region size of 2 (i.e, the height is 2) and has 2 kernels. In NLP task, we typically use kernels that slide over full rows of the matrix (words). Thus, the \"width\" of our kernels is the same as the width of the input matrix (i.e, embedding size). "]},{"cell_type":"code","metadata":{"id":"VhmO8fq9fLQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799060737,"user_tz":420,"elapsed":199,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"d41ba802-20f4-4de4-b0a7-c7e7a1744556"},"source":["convolute_region2 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(2,5), stride=1)\n","convolute_region2_output = convolute_region2(input_embeddings)\n","print(convolute_region2_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2, 6, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ihLLIBXffLQg"},"source":["The fisrt dimension is the batch size, i.e., 1.\n","\n","The second dimension is the number of output channel (number of kernels), i.e., 2.\n","\n","The third dimension is the output size of height, i.e., 6. You can use this formular to calculate:\n","$$W_{out}=\\frac{(W_{in}-K+2P)}{S}+1 = \\frac{(7-2+2\\times0)}{1}+1 = 6$$ \n","\n","The fourth dimension is the output size of width, i.e., 6. You can also use this formular to calculate:\n","$$W_{out}=\\frac{(W_{in}-K+2P)}{S}+1 = \\frac{(5-5+2\\times0)}{1}+1 = 1$$ "]},{"cell_type":"markdown","metadata":{"id":"Bww_aurefLQg"},"source":["Then, we will apply the `ReLU` function on the output of convolutional layer. "]},{"cell_type":"code","metadata":{"id":"N2HPFmqZfLQg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799062662,"user_tz":420,"elapsed":230,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"66a9ca9a-397b-4306-fcb6-84f3dd45d8d0"},"source":["ReLU_fn = nn.ReLU()\n","convolute_region2_output = ReLU_fn(convolute_region2_output)\n","print(\"convolute_region2_output:\", convolute_region2_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["convolute_region2_output: torch.Size([1, 2, 6, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"79bWLNs3fLQi"},"source":["The output size of width always is 1 because the \"width\" of our kernels is the same as embedding size in NLP task.\n","Hence, we can squeeze the last dimension. "]},{"cell_type":"code","metadata":{"id":"maJOEszqfLQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799064084,"user_tz":420,"elapsed":231,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"29664437-ca3e-494a-bb76-44a811b61a24"},"source":["convolute_region2_output = convolute_region2_output.squeeze(-1)\n","print(\"convolute_region2_output:\", convolute_region2_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["convolute_region2_output: torch.Size([1, 2, 6])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"If4ajAR8fLQl"},"source":["The second convolutional layer is region size of 3 (i.e, the height is 3) and has 2 kernels. Using the same logic, we create a layer `convolute_region3`, apply `ReLU` function, and squeeze last dimension. "]},{"cell_type":"code","metadata":{"id":"RFmUVxwefLQm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799065185,"user_tz":420,"elapsed":233,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"43bf5aaf-a7e7-4d5f-8089-1e4cec584f5b"},"source":["convolute_region3 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3,5), stride=1)\n","convolute_region3_output = convolute_region3(input_embeddings)\n","convolute_region3_output = ReLU_fn(convolute_region3_output)\n","convolute_region3_output = convolute_region3_output.squeeze(-1)\n","print(convolute_region3_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AXI9qXV7fLQo"},"source":["The third convolutional layer is region size of 4 (i.e, the height is 4) and has 2 kernels. Using the same logic, we create a layer `convolute_region4`, apply `ReLU` function, and squeeze last dimension."]},{"cell_type":"code","metadata":{"id":"Y3fdewVIfLQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799066160,"user_tz":420,"elapsed":241,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"e0ddbae6-49d2-48d3-e3e6-0d5c9571d2b8"},"source":["convolute_region4 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(4,5), stride=1)\n","convolute_region4_output = convolute_region4(input_embeddings)\n","convolute_region4_output = ReLU_fn(convolute_region4_output)\n","convolute_region4_output = convolute_region4_output.squeeze(-1)\n","print(convolute_region4_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"msvek8aSfLQq"},"source":["Note. These three layers operate on input embeddings parallelly instead of sequencially. \n","\n","Hence, instead of creating three convolutional layers separately, we can create a [`torch.nn.ModuleList()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList) to hold these submodules in a list."]},{"cell_type":"code","metadata":{"id":"SCp_PvGjfLQq"},"source":["kernel_sizes = [2,3,4] #three region size\n","kernel_num = 2      # number of kernels \n","embedding_dim = 5   # word embedding size\n","Ci = 1              # number of input channel\n","\n","convolution_layers = nn.ModuleList([\n","    nn.Conv2d(in_channels = Ci, out_channels = kernel_num, kernel_size = (K, embedding_dim)) \n","                                    for K in kernel_sizes  ])\n","\n","convolute_outputs = [F.relu(conv(input_embeddings)).squeeze(3) for conv in convolution_layers]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdMkKLT2fLQs"},"source":["`convolute_outputs` is a list which includes all the outputs from three convolutional layers.\n","Let's see how they look like. "]},{"cell_type":"code","metadata":{"id":"tEWZeg_NfLQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799068206,"user_tz":420,"elapsed":235,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"c42fd4f5-7754-4f80-8552-6f5a02945b60"},"source":["for i, item in enumerate(convolute_outputs):\n","    print(i,\" shape: \",item.shape)\n","\n","#They are same as our previous outputs. "],"execution_count":null,"outputs":[{"output_type":"stream","text":["0  shape:  torch.Size([1, 2, 6])\n","1  shape:  torch.Size([1, 2, 5])\n","2  shape:  torch.Size([1, 2, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BEtjQsIVfLQw"},"source":["#### Max Pooling layers"]},{"cell_type":"markdown","metadata":{"id":"JKD6riYQfLQx"},"source":["Our convolved outputs (maps) are size of `[batch size, number of kernels, size of height]` (e.g., `[1,2,6]`). Hence, each sample is represented in a vector by each kernel. The size of the vector is 6. \n","\n","So `1D-max pooling` is performed over each representation (i.e., the largest number from each representation is recorded). Thus a univariate feature vector is generated from all six maps."]},{"cell_type":"markdown","metadata":{"id":"z3C1tgoYfLQx"},"source":["Let's apply the [`F.max_pool1d()`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.max_pool1d) on `convolute_region2_output` fisrt. "]},{"cell_type":"code","metadata":{"id":"0OKP7AWGfLQy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799070768,"user_tz":420,"elapsed":238,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"dc770f41-2182-4b33-a18e-0fbcdee15e35"},"source":["maxpool_region2 = F.max_pool1d(convolute_region2_output, convolute_region2_output.size(2))\n","print(convolute_region2_output)\n","print(maxpool_region2)\n","print(maxpool_region2.shape) # [btach size, number of kernels, 1]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[1.0953, 0.3164, 0.0496, 0.4001, 0.0000, 0.1510],\n","         [0.4668, 0.9165, 0.3379, 0.3215, 0.1232, 0.2179]]],\n","       grad_fn=<SqueezeBackward1>)\n","tensor([[[1.0953],\n","         [0.9165]]], grad_fn=<SqueezeBackward1>)\n","torch.Size([1, 2, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8cRCa0Y-fLQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799071219,"user_tz":420,"elapsed":165,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"4c1c0577-8691-40fb-ac3c-e89d400e6833"},"source":["# We squeeze the last dimension.\n","maxpool_region2 = maxpool_region2.squeeze(-1)\n","print(maxpool_region2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"craGR5STfLQ1"},"source":["Then, same for `convolute_region3_output` and `convolute_region4_output`."]},{"cell_type":"code","metadata":{"id":"nYAn4GlxfLQ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799072211,"user_tz":420,"elapsed":243,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"65fc6ab7-7000-4fce-916d-8911d6538c3a"},"source":["maxpool_region3 = F.max_pool1d(convolute_region3_output, convolute_region3_output.size(2))\n","maxpool_region3 = maxpool_region3.squeeze(-1)\n","print(maxpool_region3.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Jsk9UhlfLQ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799072717,"user_tz":420,"elapsed":182,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"ed0e9a5f-ee0a-4553-e1f0-4107e2591ef3"},"source":["maxpool_region4 = F.max_pool1d(convolute_region4_output, convolute_region4_output.size(2))\n","maxpool_region4 = maxpool_region4.squeeze(-1)\n","print(maxpool_region4.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dF6JRqHFfLQ6"},"source":["We can also use for-loop to perform max-pooling on `convolute_outputs` that generate above by using `torch.nn.ModuleList()`. "]},{"cell_type":"code","metadata":{"id":"HWEPanAUfLQ8"},"source":["max_pooling_outputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in convolute_outputs]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OGR750FfLQ-"},"source":["`max_pooling_outputs` is a list which includes all the outputs from three max-pooling layers.\n","Let's see how they look like. "]},{"cell_type":"code","metadata":{"id":"ATk_ttUJfLQ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799074741,"user_tz":420,"elapsed":342,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"cc91ebb2-72f0-4da4-9292-7697086a9eb3"},"source":["for i, item in enumerate(max_pooling_outputs):\n","    print(i,\" shape: \",item.shape)\n","\n","#They are same as our previous outputs of max-pooling layers.  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["0  shape:  torch.Size([1, 2])\n","1  shape:  torch.Size([1, 2])\n","2  shape:  torch.Size([1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6CnJ70aufLRA"},"source":["#### Concatenate \n","\n","We concatenate the 6 features together to form a feature vector. "]},{"cell_type":"code","metadata":{"id":"hiqHXQvzfLRA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799075621,"user_tz":420,"elapsed":224,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"9eccc3d2-054f-4887-ef48-a732339cd4e9"},"source":["concat = torch.cat((maxpool_region2, maxpool_region3, maxpool_region4), dim = -1)\n","print(concat.shape) # [batch size, feature size]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 6])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cuOqUfxxfLRC"},"source":["We can also concatenate the list, `max_pooling_outputs`. "]},{"cell_type":"code","metadata":{"id":"L1DCVVnZfLRC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799076437,"user_tz":420,"elapsed":179,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"1b9a831f-f871-46c9-e821-c49b519343bb"},"source":["concat_list = torch.cat(max_pooling_outputs, 1)\n","print(concat_list.shape) # [batch size, feature size]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 6])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IhUb9SpLfLRE"},"source":["#### Dropout"]},{"cell_type":"markdown","metadata":{"id":"GHvrpGwffLRF"},"source":["Before the fully connected layers, we specify a [`drop-out layer`](https://pytorch.org/docs/stable/nn.html#dropout-layers) of 0.5 drop-out rate to avoid over-fitting in the model. "]},{"cell_type":"code","metadata":{"id":"Ll0FYsDOfLRF"},"source":["drop_out = nn.Dropout(p=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnJKrHeifLRJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799078318,"user_tz":420,"elapsed":273,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"4f5007b3-5757-4b72-ac13-fa9da1d1aa39"},"source":["drop_output = drop_out(concat)\n","print(\"drop_output:\", drop_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["drop_output: torch.Size([1, 6])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PkA1_6p5fLRK"},"source":["#### Softmax fully connected layers"]},{"cell_type":"markdown","metadata":{"id":"lbOZdrXNfLRL"},"source":[" The final **softmax fully connected layer**, then, receives this feature vector as input and uses it to classify the sentence; here it is a binary classification and hence depict two possible output states.\n"," \n","The feature size of input is `# of kernel region size X # of kernel of each region size`. \n","\n","In our case, it is `3 x 2 = 6`.\n","\n","The feature size of input is 2 (binary classification)."]},{"cell_type":"code","metadata":{"id":"YYZKBuedfLRL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799079560,"user_tz":420,"elapsed":225,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"17833729-9d69-49a6-9275-05e94dcf020d"},"source":["fc = nn.Linear(3 * 2 , 2)  \n","fc_output = F.softmax(fc(drop_output),dim=1)\n","print(\"fc_output:\", fc_output.shape)\n","print(fc_output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fc_output: torch.Size([1, 2])\n","tensor([[0.8767, 0.1233]], grad_fn=<SoftmaxBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YvyxHI_3fLRP"},"source":["The `fc_output` is the prediction of the CNN model. \n","We can give this prediction and true labels to a loss function to calculate the loss and backpropagate with the loss. "]},{"cell_type":"code","metadata":{"id":"RXEZsN_3fLRQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799080530,"user_tz":420,"elapsed":224,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"cb8f69b1-8c8c-4342-ab2a-df333e34cd37"},"source":["# We use nn.CrossEntropyLoss() as our loss function. \n","criterion = nn.CrossEntropyLoss()\n","criterion(fc_output, label_tensor)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.1392, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"Q_ldrE1AfLRR"},"source":["Congratulation! We implement the sample CNN successfully. \n","\n","Let's apply this architecture on the real-word task, sociality classification with happyDB. "]},{"cell_type":"markdown","metadata":{"id":"vyvcH5szfLRS"},"source":["## Build a `class` for CNN of text classification"]},{"cell_type":"code","metadata":{"id":"Xkn-7qYCfLRS"},"source":["# To define a CNN class\n","class CNN_Text(nn.Module):\n","    def __init__(self, vocabulary_size, embedding_dim, output_size, kernel_num, region_sizes, dropout):\n","        '''\n","        vocabulary_size: vocabulary size\n","        embedding_dim: word embedding size\n","        output_size: number of classes in prediction\n","        kernel_num: number of kernels (number of output channels of convolutional layers)\n","        region_sizes: height of kernels of convolutional layers\n","        dropout: dropout rate\n","        '''\n","        super(CNN_Text, self).__init__()\n","        # the size of input channel is 1.\n","        Ci = 1\n","        \n","        # word embedding layer\n","        self.embeddings = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_dim )\n","        \n","        # convolution with kernels\n","        self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels = Ci, out_channels = kernel_num, kernel_size = (K, embedding_dim)) for K in region_sizes])\n","        \n","        # a dropout layer\n","        self.dropout = nn.Dropout(dropout) \n","        \n","        # fully connected layer\n","        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, output_size)\n","\n","    def forward(self, x):\n","        # input x  [sequence length, batch size]\n","        \n","        input_embeddings = self.embeddings(x)  \n","        # (batch size, word_sequence, embedding_dim) word embedding\n","\n","        input_embeddings = input_embeddings.permute(1,0,2)\n","        input_embeddings = input_embeddings.unsqueeze(1)\n","        #  [batch size, number of channel is one, sequence length, embeeding size]\n","\n","        # convolutional layers\n","        convolute_outputs = [F.relu(conv(input_embeddings)).squeeze(3) for conv in self.convolution_layers]  \n","        \n","        # to get the maximum value of filtered tensor\n","        max_pooling_outputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in convolute_outputs] \n","        \n","        concat_list = torch.cat(max_pooling_outputs, 1) # concatenate representations\n","        \n","        drop_output = self.dropout(concat_list)  # add drop layer\n","        \n","        fc1_output = self.fc(drop_output)  # get the fc1 using a fully connected layer\n","        \n","        final_output = F.softmax(fc1_output,dim=1)\n","        \n","        return final_output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oN-rDOeZfLRU"},"source":["# Hyper Parameters\n","\n","# the vocabulary size\n","vocabulary_size = len(TEXT.vocab.stoi) \n","\n","# Dimension of word embedding is 300. Namely, each word is expressed by a vector that has 300 dimensions.\n","embedding_dim = 300 \n","\n","# region size as 2, 3, and 4\n","kernel_sizes = [2,3,4] \n","\n","# the number of kernel in each region size\n","kernels_num = 32  \n","\n","# The dropout rate is set to be 0.5.\n","dropout = 0.5\n","\n","# The output size of labels.\n","output_size = 2\n","\n","# learning rate is set to be 0.01.\n","lr = 0.01        \n","\n","# The number of iteration is set to be 5.\n","num_epoch = 5  \n","\n","# employ class CNN_Text and assign to cnn\n","model = CNN_Text(vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout).to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOG54Nv3fLRW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799085997,"user_tz":420,"elapsed":3347,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"6266c20d-ef2f-4b06-af02-ee6606c8a7bf"},"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.1)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","            \n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN_Text(\n","  (embeddings): Embedding(3552, 300)\n","  (convolution_layers): ModuleList(\n","    (0): Conv2d(1, 32, kernel_size=(2, 300), stride=(1, 1))\n","    (1): Conv2d(1, 32, kernel_size=(3, 300), stride=(1, 1))\n","    (2): Conv2d(1, 32, kernel_size=(4, 300), stride=(1, 1))\n","  )\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=96, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"p1ILVkdjfLRY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799089090,"user_tz":420,"elapsed":709,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"69ef5da2-ccdd-41ce-cf16-b2704c835c54"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 1,152,290 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uhIQbAAHfLRa"},"source":["# Loss and optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # define a optimizer for backpropagation\n","loss_func = nn.CrossEntropyLoss()   # define loss funtion"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxJNpz4AfLRb"},"source":["`train()` and `evaluate()` functions"]},{"cell_type":"code","metadata":{"id":"TbvmYNIZfLRb"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    model.train()\n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        batch_input, labels = batch.tweet, batch.label\n","        batch_input = batch_input.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        \n","        outputs = model(batch_input)\n","\n","        loss = criterion(outputs, labels)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.cpu().item()\n","\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XDsvKHZfLRe"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    all_pred=[]\n","    all_label = []\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            batch_input, labels = batch.tweet, batch.label\n","            batch_input = batch_input.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(batch_input)\n","\n","            loss = criterion(outputs, labels)\n","\n","            epoch_loss += loss.cpu().item()\n","\n","            # identify the predicted class for each example in the batch\n","            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n","            # put all the true labels and predictions to two lists\n","            all_pred.extend(predicted)\n","            all_label.extend(labels.cpu())\n","    \n","    accuracy = accuracy_score(all_label, all_pred)\n","    f1score = f1_score(all_label, all_pred, average='macro') \n","    return epoch_loss / len(iterator), accuracy, f1score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyCTv64wfLRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616799327292,"user_tz":420,"elapsed":9624,"user":{"displayName":"chiyu zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqrbU-_LsK0bRS1xN21hE7Y6_yasHnFNDAGMna=s64","userId":"09517051177095071345"}},"outputId":"69cbc000-50bf-40d0-f4d8-84422202b75a"},"source":["# Train the model\n","MAX_EPOCH = 15\n","total_step = len(train_iter)\n","loss_list = []\n","acc_list = []\n","\n","for epoch in trange(MAX_EPOCH, desc=\"Epoch\"):\n","    train_loss = train(model, train_iter, optimizer, criterion)  \n","    val_loss, val_acc, val_f1 = evaluate(model, val_iter, criterion)\n","\n","    # Create checkpoint at end of each epoch\n","    state_dict_model = model.state_dict() \n","    state = {\n","        'epoch': epoch,\n","        'state_dict': state_dict_model,\n","        'optimizer': optimizer.state_dict()\n","        }\n","\n","    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_cnn/CNN_TEXT_\"+str(epoch+1)+\".pt\")\n","\n","    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, MAX_EPOCH, train_loss, val_loss, val_acc, val_f1))\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:   7%|▋         | 1/15 [00:00<00:11,  1.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [1/15], Train Loss: 0.4036, Validation Loss: 0.4269, Validation Accuracy: 0.8883, Validation F1: 0.8876\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  13%|█▎        | 2/15 [00:01<00:09,  1.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [2/15], Train Loss: 0.4110, Validation Loss: 0.4253, Validation Accuracy: 0.8902, Validation F1: 0.8894\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 3/15 [00:02<00:08,  1.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [3/15], Train Loss: 0.4070, Validation Loss: 0.4246, Validation Accuracy: 0.8911, Validation F1: 0.8908\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  27%|██▋       | 4/15 [00:02<00:07,  1.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [4/15], Train Loss: 0.4094, Validation Loss: 0.4148, Validation Accuracy: 0.8968, Validation F1: 0.8962\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 5/15 [00:03<00:06,  1.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [5/15], Train Loss: 0.4127, Validation Loss: 0.4148, Validation Accuracy: 0.8835, Validation F1: 0.8821\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 6/15 [00:03<00:05,  1.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [6/15], Train Loss: 0.4087, Validation Loss: 0.4144, Validation Accuracy: 0.8835, Validation F1: 0.8821\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  47%|████▋     | 7/15 [00:04<00:05,  1.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [7/15], Train Loss: 0.4284, Validation Loss: 0.4312, Validation Accuracy: 0.8902, Validation F1: 0.8895\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  53%|█████▎    | 8/15 [00:05<00:04,  1.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [8/15], Train Loss: 0.4395, Validation Loss: 0.4489, Validation Accuracy: 0.8684, Validation F1: 0.8681\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 9/15 [00:05<00:03,  1.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [9/15], Train Loss: 0.5892, Validation Loss: 0.4795, Validation Accuracy: 0.8381, Validation F1: 0.8378\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 10/15 [00:06<00:03,  1.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [10/15], Train Loss: 0.4263, Validation Loss: 0.4252, Validation Accuracy: 0.8911, Validation F1: 0.8907\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  73%|███████▎  | 11/15 [00:06<00:02,  1.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [11/15], Train Loss: 0.4203, Validation Loss: 0.4608, Validation Accuracy: 0.8674, Validation F1: 0.8665\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 12/15 [00:07<00:01,  1.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [12/15], Train Loss: 0.4205, Validation Loss: 0.4490, Validation Accuracy: 0.8750, Validation F1: 0.8736\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  87%|████████▋ | 13/15 [00:08<00:01,  1.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [13/15], Train Loss: 0.4078, Validation Loss: 0.4243, Validation Accuracy: 0.8920, Validation F1: 0.8918\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  93%|█████████▎| 14/15 [00:08<00:00,  1.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [14/15], Train Loss: 0.4182, Validation Loss: 0.4748, Validation Accuracy: 0.8371, Validation F1: 0.8301\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 15/15 [00:09<00:00,  1.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Epoch [15/15], Train Loss: 0.4314, Validation Loss: 0.4330, Validation Accuracy: 0.8949, Validation F1: 0.8944\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"e02BYEB-fLRi"},"source":["# Exercise "]},{"cell_type":"markdown","metadata":{"id":"qZh_Y144fLRl"},"source":["* Can you implement a Convolutional Reccurent Neural Networks with the following architecture?\n","```\n","CLSTM(\n","  (convs1): ModuleList(\n","    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n","    (1): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(2, 0))\n","  )\n","  (lstm): LSTM(200, 200, batch_first=True)\n","  (hidden2label): Linear(in_features=200, out_features=2, bias=True)\n","  (softmax): LogSoftmax()\n","  (dropout_layer): Dropout(p=0.2)\n",")\n","```"]},{"cell_type":"markdown","metadata":{"id":"lKUXAm3yfLRl"},"source":["### Reference\n","* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n","* https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n","* http://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/"]},{"cell_type":"code","metadata":{"id":"HXXUjiG9wAdO"},"source":[""],"execution_count":null,"outputs":[]}]}